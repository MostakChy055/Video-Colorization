# -*- coding: utf-8 -*-
"""arch_util.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wQSICGIau99NvBUqZipIAcDsWYtgO1kM
"""

# This code defines several helper functions and classes commonly used in convolutional neural networks (CNNs) for computer vision tasks.
import torch
import torch.nn as nn
import torch.nn.init as init
import torch.nn.functional as F

import numpy as np

# This function initializes the weights of a network with Kaiming Normal initialization with a=0 and fan_in mode. It also applies a scaling factor (scale) to the weights for residual blocks. Additionally, it initializes biases in convolutional layers and batch normalization layers to zero.
def initialize_weights(net_l, scale=1):
    # This line checks if the input net_l is a list. If not, it converts it to a list. This allows the function to handle both individual network modules and a list of modules for weight initialization.
    if not isinstance(net_l, list):
        net_l = [net_l]

    # Iterating through each newwork and it's modules
    for net in net_l:
        for m in net.modules():
            if isinstance(m, nn.Conv2d):
                # Kaiming Normal initialization is a popular weight initialization technique for deep neural networks, especially those with ReLU activations. It aims to preserve the variance of activations throughout the network, preventing vanishing or exploding gradients.
                #  This mode scales the weights based on the number of incoming connections to a neuron. It's generally preferred for convolutional layers where the number of input channels is typically much larger than the number of output channels.
                init.kaiming_normal_(m.weight, a=0, mode="fan_in")
                m.weight.data *= scale  # for residual block
                if m.bias is not None:
                    m.bias.data.zero_()
            elif isinstance(m, nn.Linear):
                init.kaiming_normal_(m.weight, a=0, mode="fan_in")
                m.weight.data *= scale
                if m.bias is not None:
                    m.bias.data.zero_()
            elif isinstance(m, nn.BatchNorm2d):
                init.constant_(m.weight, 1)
                init.constant_(m.bias.data, 0.0)


# This function takes a building block (e.g., a convolutional block) and a number of layers (n_layers) as input. It creates a sequential layer by stacking n_layers instances of the provided building block. This is a convenient way to create repeated patterns of layers in a network.
def make_layer(block, n_layers):
    layers = []
    for _ in range(n_layers):
        layers.append(block())
    return nn.Sequential(*layers)


# This class defines a residual block without Batch Normalization (BN). It consists of two convolutional layers with ReLU activation in between. The output of the second convolutional layer is added to the original input using a residual connection. This type of block is commonly used in deep networks to improve gradient flow and prevent vanishing gradients.
class ResidualBlock_noBN(nn.Module):
    def __init__(self, nf=64):
        super(ResidualBlock_noBN, self).__init__()
        self.conv1 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)
        self.conv2 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)

        # initialization
        initialize_weights([self.conv1, self.conv2], 0.1)

    def forward(self, x):
        identity = x
        out = F.relu(self.conv1(x), inplace=True)
        out = self.conv2(out)
        return identity + out


# This class defines a residual block with Batch Normalization (BN). Residual mapping is essentially the difference between the output of the network and the input.
class ResidualBlock(nn.Module):
    def __init__(self, nf=64):
        super(ResidualBlock, self).__init__()
        self.conv1 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)
        self.bn = nn.BatchNorm2d(nf)
        self.conv2 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)

        # initialization
        initialize_weights([self.conv1, self.conv2], 0.1)

    def forward(self, x):
        identity = x
        out = F.relu(self.bn(self.conv1(x)), inplace=True)
        out = self.conv2(out)

        #  By adding the original input to the output of the block, it creates a direct path for gradients to flow back through the network. This helps to alleviate the vanishing gradient problem, especially in deeper networks. In some cases, the optimal mapping between input and output might simply be an identity function (i.e., the output should be the same as the input). With a residual connection, the network can learn to set the weights of the convolutional layers such that the output of the block is close to zero, effectively learning the identity mapping.
        return identity + out


# Warp an image or feature map with optical flow. The flow_warp function you provided calculates the new positions of pixels based on the optical flow, and then uses interpolation to determine the pixel values at those new positions. This process effectively shifts the image according to the motion information encoded in the optical flow.
def flow_warp(x, flow, interp_mode="bilinear", padding_mode="zeros"):
    """
    Args:
        x (Tensor): size (N, C, H, W)
        flow (Tensor): size (N, H, W, 2), normal value
        interp_mode (str): 'nearest' or 'bilinear'
        padding_mode (str): 'zeros' or 'border' or 'reflection'

    Returns:
        Tensor: warped image or feature map
    """

    # Checks if the spatial dimensions of the input image and flow match.
    assert x.size()[-2:] == flow.size()[1:3]
    B, C, H, W = x.size()

    grid_y, grid_x = torch.meshgrid(torch.arange(0, H), torch.arange(0, W))
    # Stacks the x and y coordinates into a single tensor with shape (H, W, 2).
    grid = torch.stack((grid_x, grid_y), 2).float()
    # This means that the gradients with respect to grid will not be computed during backpropagation. Since the grid is a fixed coordinate system, it doesn't need to be updated during optimization.
    grid.requires_grad = False
    # This line ensures that the grid tensor has the same data type as the input image x. This is important for efficient computation and to avoid potential type-related errors.
    grid = grid.type_as(x)
    # This line calculates the warped grid coordinates by adding the optical flow flow to the original grid grid. The resulting vgrid tensor represents the new positions of the pixels after applying the flow field.
    vgrid = grid + flow

    # This line extracts the x-coordinates from the vgrid tensor and scales them to the range [-1, 1]. This is necessary because the grid_sample function expects coordinates in this range. The max(W - 1, 1) part ensures that the denominator is always at least 1 to avoid division by zero.
    vgrid_x = 2.0 * vgrid[:, :, :, 0] / max(W - 1, 1) - 1.0
    vgrid_y = 2.0 * vgrid[:, :, :, 1] / max(H - 1, 1) - 1.0
    #
    vgrid_scaled = torch.stack((vgrid_x, vgrid_y), dim=3)

    # This line performs the actual image warping using the grid_sample function. It takes the input image x and the scaled grid vgrid_scaled as input. The interp_mode and padding_mode arguments specify how to handle interpolation and out-of-bounds pixels. The function returns the warped image output.
    output = F.grid_sample(x, vgrid_scaled, mode=interp_mode, padding_mode=padding_mode)
    return output