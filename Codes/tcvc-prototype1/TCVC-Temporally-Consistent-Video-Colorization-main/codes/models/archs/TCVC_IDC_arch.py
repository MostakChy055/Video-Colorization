# -*- coding: utf-8 -*-
"""TCVC_IDC_arch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fuXk1fm8lsbC7saJN36K9b0qgIyTaPNa
"""

# Commented out IPython magic to ensure Python compatibility.
def initializeRaft():
  !git clone https://github.com/princeton-vl/RAFT.git
#   %cd RAFT
  import os
  os.chdir('core')
  import torch
  from raft import *  # Assuming raft.py is saved as a module


  import argparse

  parser = argparse.ArgumentParser()
  parser.add_argument('--name', default='raft', help="name your experiment")
  parser.add_argument('--stage', help="determines which dataset to use for training")
  parser.add_argument('--restore_ckpt', help="restore checkpoint")
  parser.add_argument('--small', action='store_true', help='use small model')
  parser.add_argument('--validation', type=str, nargs='+')

  parser.add_argument('--lr', type=float, default=0.00002)
  parser.add_argument('--num_steps', type=int, default=100000)
  parser.add_argument('--batch_size', type=int, default=6)
  parser.add_argument('--image_size', type=int, nargs='+', default=[384, 512])
  parser.add_argument('--gpus', type=int, nargs='+', default=[0,1])
  parser.add_argument('--mixed_precision', action='store_true', help='use mixed precision')

  parser.add_argument('--iters', type=int, default=12)
  parser.add_argument('--wdecay', type=float, default=.00005)
  parser.add_argument('--epsilon', type=float, default=1e-8)
  parser.add_argument('--clip', type=float, default=1.0)
  parser.add_argument('--dropout', type=float, default=0.0)
  parser.add_argument('--gamma', type=float, default=0.8, help='exponential weighting')
  parser.add_argument('--add_noise', action='store_true')

  # Simulate command-line arguments as strings
  args_list = [
      '--name', 'raft',
      '--restore_ckpt', '/content/drive/MyDrive/models/raft-kitti.pth',
      '--lr', '0.00002',
      '--num_steps', '100000',
      '--batch_size', '6',
      '--image_size', '300', '300',
      '--gpus', '0',
      '--iters', '12',
      '--wdecay', '0.00005',
      '--epsilon', '1e-8',
      '--clip', '1.0',
      '--gamma', '0.8'
  ]

  # Use parse_args with the simulated arguments
  args = parser.parse_args(args_list)
  RaftModel = nn.DataParallel(RAFT(args), device_ids=args.gpus)
  print(f"Loading pretrained weights from {args.restore_ckpt}")
  checkpoint = torch.load(args.restore_ckpt, map_location=torch.device('cuda'))
  RaftModel.load_state_dict(checkpoint)
  RaftModel.eval()

  return RaftModel

# This code implements a video frame interpolation model named "TCVC_IDC". It aims to generate intermediate frames between a pair of keyframes in a video sequence.

import torch
import torch.nn as nn
import torch.nn.functional as F
import functools
from collections import OrderedDict

from models.archs import arch_util

#### keyframe branches
from models.archs.colorizers.siggraph17 import siggraph17
from models.archs.colorizers.eccv16 import


#### Flow estimation
from models.archs.networks.FlowNet2 import FlowNet2
from models.archs.flow_vis import *
from models.archs.networks.resample2d_package.resample2d import Resample2d

# This class takes a feature map as input and predicts a weight map used for combining features from different stages.
class WeightingNet(nn.Module):
    def __init__(self, input=352, output=1):
        super(WeightingNet, self).__init__()
        self.conv1 = nn.Conv2d(input, 196, 3, padding=1)
        self.conv2 = nn.Conv2d(196, 64, 3, padding=1)
        self.conv3 = nn.Conv2d(64, output, 3, padding=1)

    def forward(self, x):
        x = F.leaky_relu(self.conv1(x), negative_slope=0.1)
        x = F.leaky_relu(self.conv2(x), negative_slope=0.1)
        x = self.conv3(x)
        return x


# This class takes a feature map as input and refines it for better quality.
class Feature_Refine(nn.Module):
    def __init__(self, input=288, output=128):
        super(Feature_Refine, self).__init__()
        self.conv1 = nn.Conv2d(input, 196, 3, padding=1)
        self.conv2 = nn.Conv2d(196, 128, 3, padding=1)
        self.conv3 = nn.Conv2d(128, output, 3, padding=1)

    def forward(self, x):
        x = F.leaky_relu(self.conv1(x), negative_slope=0.1)
        x = F.leaky_relu(self.conv2(x), negative_slope=0.1)
        x = self.conv3(x)
        return x

# This class reduces the number of channels in a feature map.
class Channel_Reduction(nn.Module):
    def __init__(self, input, output):
        super(Channel_Reduction, self).__init__()
        self.conv1 = nn.Conv2d(input, output, 1, padding=0)
        self.conv2 = nn.Conv2d(output, output, 3, padding=1)
        self.conv3 = nn.Conv2d(output, output, 1, padding=0)

    def forward(self, x):
        x = F.leaky_relu(self.conv1(x), negative_slope=0.1)
        x = F.leaky_relu(self.conv2(x), negative_slope=0.1)
        x = self.conv3(x)
        return x

class Channel_Reduction_1x1(nn.Module):
    def __init__(self, input, output):
        super(Channel_Reduction_1x1, self).__init__()
        self.conv1 = nn.Conv2d(input, output, 1, padding=0)

    def forward(self, x):
        x = self.conv1(x)

        return x

class flownet_options():
    def __init__(self):
        super(flownet_options, self).__init__()
        self.rgb_max = 1.0
        self.fp16 = False

# nf: Number of filters in convolutional layers (default: 64).
# N_RBs: Number of residual blocks (not used in the provided code).
class TCVC_IDC(nn.Module):
    def __init__(
        self, nf = 64, N_RBs = 2, key_net = "sig17", dataset = "DAVIS", train_flow_keyNet = False
    ):
        super(TCVC_IDC, self).__init__()

        self.key_net = key_net

        self.L_fea_extractor = nn.Sequential(
            nn.Conv2d(1, 32, 3, 1, 1, bias=True),
            nn.LeakyReLU(0.1),
            nn.Conv2d(32, 32, 3, 1, 1, bias=True)
        )

        self.trainig = False
        self.channel_reduction = Channel_Reduction(128, 64).cuda()
        self.weigting = F(32*3 + 128*2, 1).cuda()
        self.feature_refine = Feature_Refine(32 * 3 + 64 * 3, 128).cuda()
        self.need_conv = False


        # define keyframe branch
        if key_net == "sig17":
            self.fea_key = siggraph17()
            nf_key = 128
        else:
            raise NotImplementedError("Currently only support Sig17")

        # This line sets the model in evaluation mode. When a model is in evaluation mode, certain behaviors change: Dropout layers are bypassed (no dropout is applied), Batch normalization uses the running average statistics instead of calculating them for each batch.
        if self.training:
            self.fea_key.train()
        else:
            self.fea_key.eval()


        #### SPyNet for flow warping
#         self.flow = spynet.SpyNet()

        opts = flownet_options()
        self.flow = initializeRaft()
        self.flow.eval()
        self.flow_warping = Resample2d()

        self.sigmoid = nn.Sigmoid()
        self.MSE = nn.L1Loss(size_average = True)

        # for training only the interframe branch
        # If train_flow_keyNet is False, freezes the parameters of the keyframe extraction network and the flow estimation network.
        if not train_flow_keyNet:
            for param in self.fea_key.parameters():
                param.requires_grad = False
            for param in self.flow.parameters():
                param.requires_grad = False



    # This method takes input frames and potentially features from the first keyframe and performs the core processing of the model
    def forward(self, x, first_key_HR = None, first_key_fea = None):
        """Assuming M + 2 frames with keyframes at two end points
        input:
            x: LR frames
                - [(B, N, nf, H, W), (B, N, nf, H, W), ..., (B, N, nf, H, W), (B, N, nf, H, W)]
                - In total M + 2 entries
                - N: depends on the keyframe branch
            first_key_HR: HR output of the first keyframe - (B, 3, H, W)
            first_key_fea: features of the first keyframe for forward prop. - (B, nf, H, W)
        output:
            out: output HR frames - (B, N + 1, 3, H, W)
            last_key_HR: HR output of the last keyframe - (B, 3, H, W)
            fea_backward_output: features of the last keyframe - (B, nf, H, W)
        """

        # Accesses the first frame from the input sequence x. Retrieves the size (dimensions) of the first frame.
        B, C, H, W = x[0].size()  # N frames, C = 1. Low Resolution frame.

        if self.training:
            self.fea_key.train()
        else:
            self.fea_key.eval()


        # This sets the fea_key network (used for keyframe feature extraction) to evaluation mode. This typically disables features like dropout that might be used during training.
        # first key frame
        x_p = x[0].to_key()
        if first_key_fea is not None and first_key_HR is not None:
            key_p_HR, fea_forward = first_key_HR, first_key_fea
        else:
            key_p_HR, fea_forward = self.fea_key(x_p)
        out_l = []
        out_l.append(key_p_HR)

        # last key frame
        x_n = x[- 1]
        last_key_HR, fea_backward = self.fea_key(x_n)

        #### backward propagation
        backward_fea_l = []
        backward_fea_l.insert(0, fea_backward)
        for i in range(len(x) - 2, 0, -1):
            x_n = x[i + 1]
            x_current = x[i]

            # it repeats the tensor 3 times along the second dimension (channels), effectively converting the grayscale image into a 3-channel (RGB) image.
            flow = self.flow( x_current.repeat(1, 3, 1, 1) + 0.5, x_n.repeat(1, 3, 1, 1) + 0.5 )
            #fea_backward = arch_util.flow_warp(fea_backward, flow.permute(0, 2, 3, 1))    # init
            fea_backward = self.flow_warping(fea_backward, flow)

            input_ = fea_backward

            fea_backward = input_

            backward_fea_l.insert(0, fea_backward)


        #### forward propagation
        ab_fwarp_l = []
        non_mask_fwarp_l = []

        x_current = x[0]
        x_n = x[1]
        flow_n_c = self.flow(x_n.repeat(1, 3, 1, 1) + 0.5, x_current.repeat(1, 3, 1, 1) + 0.5)

        # making sure that the grayscale content (the shapes and objects) of the current frame (x_current) is aligned with the next frame (x_n). It's like shifting the black-and-white outline of a drawing so that it matches with the next frame in the sequence, accounting for any movement. This is purely about structure—no color involved yet.
        warp_x_c = self.flow_warping(x_current, flow_n_c)
        # ensuring that the color information from the keyframe (where the model first decided on colors) is correctly transferred to the current frame. It's like taking the color paint from the first frame and carefully applying it to the corresponding areas in the current frame, even if those areas have moved. This step is about making sure the color remains consistent across frames despite any motion.
        ab_fwarp = self.flow_warping(key_p_HR, flow_n_c) # [B, 2, H, W]

        ab_fwarp_l.append(ab_fwarp)
        # Is a measure of how well the warped frame (warp_x_c) aligns with the actual next frame (x_n). helps the model understand which areas of the warped frame are trustworthy and should contribute to the final colorization.
        non_mask = torch.exp( -50 * torch.sum(x_n - warp_x_c, dim=1).pow(2) ).unsqueeze(1)
        non_mask_fwarp_l.append(non_mask)

        forward_fea_l = []
        forward_fea_l.append(fea_forward)

        # This code block iterates through the intermediate frames (excluding the first and last keyframes) and performs the core interpolation steps
        for i in range(1, len(x) - 1):
            x_p = x[i - 1]
            x_current = x[i]
            x_n = x[i + 1]

            x_p_fea = self.L_fea_extractor(x_p)
            x_c_fea = self.L_fea_extractor(x_current)
            x_n_fea = self.L_fea_extractor(x_n)

            flow = self.flow(x_current.repeat(1, 3, 1, 1) + 0.5, x_p.repeat(1, 3, 1, 1) + 0.5)
            fea_forward = self.flow_warping(fea_forward, flow)

            # This step combines features from both the forward and backward directions using a learned weighting mechanism. The weighting network (W) determines the importance of the forward and backward features at each pixel. By blending these features, the model can better handle motion and ensure that both past and future frames contribute to the current frame's colorization. This helps in maintaining temporal consistency, reducing artifacts that may arise from using only one direction.
            W = torch.sigmoid(self.weigting(torch.cat([x_p_fea, x_c_fea, x_n_fea, backward_fea_l[i - 1], fea_forward], dim=1)))  # [B, 1, H, W]   in_c: 32*3+128+128
            input_ =  W * fea_forward +  (1 - W) * backward_fea_l[i - 1]  # [B, 128, H, W]

            # It reduces the dimensionality of the backward, forward, and fusion features, making them more manageable and focused on the most important information.
            x_n_backward_fea = self.channel_reduction(backward_fea_l[i].detach())
            x_p_forward_fea = self.channel_reduction(forward_fea_l[i-1].detach())
            x_c_fusion_fea = self.channel_reduction(input_.detach())
            # The concatenated features contain a mix of forward, backward, and fusion features, along with extracted features from the original frames (x_p_fea, x_c_fea, x_n_fea). The feature_refine network processes these combined features to create a refined representation.
            fea_residual = self.feature_refine(torch.cat([x_p_fea, x_c_fea, x_n_fea, x_n_backward_fea, x_p_forward_fea, x_c_fusion_fea], dim=1))

            # This helps in preserving the original feature information while also introducing new refined details. It is crucial for capturing subtle details that might be missed in the initial fusion.
            fea_forward = input_ + fea_residual


            # This step refines the color information based on the refined features. The output here represents the colorized version of the current frame, which should be consistent with the surrounding frames and retain high-quality color details.
            out = self.fea_key.model_out(fea_forward)

            out_l.append(out)

            flow_n_c = self.flow(x_n.repeat(1, 3, 1, 1) + 0.5, x_current.repeat(1, 3, 1, 1) + 0.5)
            warp_x_c = self.flow_warping(x_current, flow_n_c)
            ab_fwarp = self.flow_warping(out, flow_n_c) # [B, 2, H, W]


            ab_fwarp_l.append(ab_fwarp)

            non_mask = torch.exp( -50 * torch.sum(x_n - warp_x_c, dim=1).pow(2) ).unsqueeze(1)
            non_mask_fwarp_l.append(non_mask)

            forward_fea_l.append(fea_forward)

        out_l.append(last_key_HR)
        # This means that the new dimension will be inserted at the first position (0-indexed) in the tensor.  By stacking along dim = 1, the model can handle the temporal aspect of the video data effectively. This allows for operations that need to be applied across all frames, such as calculating temporal consistency losses or feeding the stacked tensor into a network that processes sequences of frames.
        out = torch.stack(out_l, dim=1)
        ab_fwarp_l_stack = torch.stack(ab_fwarp_l, dim=1)  # [B, N-2, H, W]
        non_mask_fwarp_l_stack = torch.stack(non_mask_fwarp_l, dim=1)  # [B, N-2, H, W]

        # warp loss with t=2
        if len(x) >= 3:
            warp_t = 2
            ab_fwarp_l_2 = []
            non_mask_fwarp_l_2 = []
            #  By calculating the optical flow between frames that are two steps apart (rather than consecutive frames), the model captures motion information over a longer time span. This is particularly important for video sequences where objects may move significantly between frames, or where motion is not consistent frame-to-frame (e.g., sudden movements).
            for i in range(0, len(x) - warp_t, 1):
                x_current = x[i]
                x_n = x[i+warp_t]
                #  Calculates the flow between x_n and x_current
                # The repeat(1, 3, 1, 1) operation replicates the input tensors along the second dimension (channels) three times. This is likely done to match the expected input format of the self.flow module.
                flow_n_c = self.flow(x_n.repeat(1, 3, 1, 1)+0.5, x_current.repeat(1, 3, 1, 1)+0.5)
                warp_x_c = self.flow_warping(x_current, flow_n_c)
                ab_fwarp = self.flow_warping(out_l[i], flow_n_c) # [B, 2, H, W]
                ab_fwarp_l_2.append(ab_fwarp)

                # The non-mask value is essentially a confidence measure of how well a warped frame aligns with the original frame.
                # It's computed by measuring the pixel-wise difference between the original frame and its warped counterpart. The smaller the difference, the higher the non-mask value, indicating a better alignment. Difference between current frame and the predicted frame.
                # It helps us in Evaluating warping accuracy, Identifying potential issues
                # x_n - warp_x_c computes the pixel-wise difference between the original frame x_n and the warped frame warp_x_c then sums the squared differences across the color channels (RGB) for each pixel and further applies an exponential function to the summed differences. This transforms the values into a range between 0 and 1, with larger differences resulting in smaller values and finally adds a new dimension to match the shape of the other tensors in the computation.
                non_mask = torch.exp( -50 * torch.sum(x_n - warp_x_c, dim=1).pow(2) ).unsqueeze(1)
                non_mask_fwarp_l_2.append(non_mask)

            # By stacking along dimension 1, we create a tensor that efficiently stores all the warped frames for a given batch.
            ab_fwarp_l_stack_2 = torch.stack(ab_fwarp_l_2, dim=1)  # [B, N-2, H, W]
            non_mask_fwarp_l_stack_2 = torch.stack(non_mask_fwarp_l_2, dim=1)  # [B, N-2, H, W]
        else:
            ab_fwarp_l_stack_2 = None
            non_mask_fwarp_l_stack_2 = None


        return out, ab_fwarp_l_stack, non_mask_fwarp_l_stack, ab_fwarp_l_stack_2, non_mask_fwarp_l_stack_2

# Overall Workflow

# The model receives a sequence of low-resolution frames (and optionally the first keyframe's HR output and features).
# It extracts features for the first keyframe if necessary.
# It performs backward feature propagation from the last keyframe to intermediate frames.
# It performs forward feature propagation from the first keyframe to intermediate frames.
# At each stage, it combines and refines features from the forward and backward passes.
# It generates the interpolated frames using the refined features.
# It returns the interpolated frames and additional information like warped keyframe features for potential loss calculation during training.

Yes, that's correct! Here's a step-by-step breakdown of what happens:

### 1. **Two Consecutive Frames:**
   - Let's say you have two grayscale frames: **Frame 1** and **Frame 2**.

### 2. **Calculating Optical Flow:**
   - The model computes the optical flow between these two frames. Optical flow is a vector field that represents the movement of objects (or pixels) from Frame 1 to Frame 2.
   - **Example:** If an object (like the red ball from our earlier example) moves 10 pixels to the right from Frame 1 to Frame 2, the optical flow will reflect that movement.

### 3. **Warping the Features:**
   - The model takes the features (including color information) from Frame 1 and warps them using the optical flow to align them with Frame 2.
   - **Result:** The color information from Frame 1 is shifted according to the motion detected by the optical flow. So if the red ball moved to the right, the red color associated with the ball in Frame 1 is shifted to the ball's new position in Frame 2.

### 4. **Color Information Transfer:**
   - This warped color information is then used to help colorize Frame 2.
   - **Example:** The model uses the shifted red color to ensure that the ball in Frame 2 is correctly colored red in its new position.

### 5. **Combining with Current Frame:**
   - The warped color information is combined with the original grayscale information of Frame 2. This combination helps the model make a more accurate color prediction for Frame 2.
   - The model refines the colorization to ensure consistency and accuracy, especially at the boundaries of moving objects.

### 6. **Result:**
   - The final colorized version of Frame 2 now has color information that is consistent with Frame 1, thanks to the warping process based on optical flow.

### **Summary:**
- The warping process ensures that the color from Frame 1 is smoothly transferred to the corresponding areas in Frame 2, accounting for any movement between the frames. This helps maintain temporal consistency, making the video look natural and preventing color flickering or inconsistencies between frames.

import torch
import torch.nn.functional as F

def flow_warping(feature, flow):
    """
    Warps the input feature map using the provided flow field.

    Parameters:
    - feature: The feature map to be warped (BxCxHxW).
    - flow: The optical flow field (Bx2xHxW) where 2 corresponds to (dx, dy) flow vectors.

    Returns:
    - Warped feature map (BxCxHxW).
    """
    B, C, H, W = feature.size()

    # Create mesh grid
    grid_y, grid_x = torch.meshgrid(torch.arange(0, H), torch.arange(0, W))
    grid_x = grid_x.float().to(flow.device)
    grid_y = grid_y.float().to(flow.device)

    # Normalize the grid coordinates to the range [-1, 1]
    grid_x = (2.0 * grid_x / (W - 1)) - 1.0
    grid_y = (2.0 * grid_y / (H - 1)) - 1.0

    # Add flow to grid coordinates
    flow_x = flow[:, 0, :, :]  # Flow in x direction
    flow_y = flow[:, 1, :, :]  # Flow in y direction
    warped_grid_x = grid_x[None, :, :].expand_as(flow_x) + flow_x
    warped_grid_y = grid_y[None, :, :].expand_as(flow_y) + flow_y

    # Stack grid and reshape to (B, H, W, 2)
    warped_grid = torch.stack((warped_grid_x, warped_grid_y), dim=-1)
    warped_grid = warped_grid.permute(0, 2, 3, 1)  # Reshape for grid_sample (B, H, W, 2)

    # Warp the feature map
    warped_feature = F.grid_sample(feature, warped_grid, mode='bilinear', padding_mode='border', align_corners=True)

    return warped_feature