{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10873775,"sourceType":"datasetVersion","datasetId":6756103},{"sourceId":12359694,"sourceType":"datasetVersion","datasetId":7792400}],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!git clone https://github.com/princeton-vl/RAFT.git\n%cd RAFT\nimport os\nos.chdir('core')\nimport torch\nfrom raft import *  # Assuming raft.py is saved as a module","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport torch\nfrom raft import RAFT\nimport argparse\nfrom scipy.ndimage import laplace\nimport torch.nn.functional as F\nimport torch.nn as nn\nfrom tqdm import tqdm\n\nclass VideoProcessor:\n    def __init__(self, bin_sizes=[32, 64, 128], embed_dim=64):\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.model = self._init_raft()\n        self.resize_dim = (256, 256)\n        \n        self.bin_sizes = bin_sizes\n        self.max_bins = max(bin_sizes)\n        self.embed_dim = embed_dim\n        \n        # Initialize histogram network\n        self._init_histogram_network()\n                \n        \n    def _init_raft(self):\n        args = argparse.Namespace(\n            name='raft', stage='chairs', validation='kitti',\n            mixed_precision=False, small=False, dropout=0,\n            alternate_corr=False, model='',\n            restore_ckpt='/kaggle/input/vcdataset/models/raft-kitti.pth',\n            path='', gpus=[0], iters=12\n        )\n        model = RAFT(args)\n        checkpoint = torch.load(args.restore_ckpt, map_location=self.device, weights_only=True)  # Added weights_only=True for security\n        model.load_state_dict(checkpoint, strict=False)\n        return model.to(self.device).eval()\n\n    def _init_histogram_network(self):\n        \"\"\"Initialize learnable histogram processing layers\"\"\"\n        self.fc_layers = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(bins, self.embed_dim),\n                nn.ReLU(),\n                nn.LayerNorm(self.embed_dim)\n            ) for bins in self.bin_sizes\n        ]).to(self.device)\n        \n        self.attention = nn.MultiheadAttention(\n            self.embed_dim, num_heads=4\n        ).to(self.device)\n        self.output_proj = nn.Linear(self.embed_dim, self.embed_dim).to(self.device)\n\n    def _process_histograms(self, frame):\n        \"\"\"Process frame histograms with attention\"\"\"\n        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n        \n        # Calculate all histograms\n        hists = [\n            cv2.normalize(\n                cv2.calcHist([gray], [0], None, [bins], [0, 256]),\n                None\n            ).flatten() \n            for bins in self.bin_sizes\n        ]\n        \n        # Convert to tensor and process\n        hist_tensors = [torch.from_numpy(h).float().to(self.device) for h in hists]\n        \n        # Embed each histogram\n        embeddings = []\n        for hist, fc in zip(hist_tensors, self.fc_layers):\n            embeddings.append(fc(hist))\n        \n        # Attention processing\n        embeddings = torch.stack(embeddings)  # [num_scales, embed_dim]\n        attn_out, _ = self.attention(embeddings, embeddings, embeddings)\n        return self.output_proj(attn_out.mean(dim=0)).detach()  # Added detach() here\n\n    def calculate_temporal_sharpness(self, l_channel_sequence):\n        sharpness_scores = []\n        for i in range(1, len(l_channel_sequence)):\n            # Current and previous frame\n            prev = l_channel_sequence[i-1].astype(np.float32)\n            curr = l_channel_sequence[i].astype(np.float32)\n            \n            # Frame difference (temporal gradient)\n            temporal_diff = cv2.absdiff(curr, prev)\n            \n            # Spatial sharpness (Laplacian variance)\n            spatial_sharpness = laplace(curr).var()\n            \n            # Combined metric (adjust weights as needed)\n            combined = 0.7 * temporal_diff.mean() + 0.3 * spatial_sharpness\n            sharpness_scores.append(combined)\n            \n        return np.array(sharpness_scores)\n    \n    def process_folder(self, input_folder, output_base):\n        \"\"\"Modified to include sharpness calculation\"\"\"\n        frame_paths = sorted([f for f in os.listdir(input_folder) \n                           if f.endswith(('.png','.jpg','.jpeg'))])\n        \n        # Create output directories\n        os.makedirs(os.path.join(output_base, 'L'), exist_ok=True)\n        os.makedirs(os.path.join(output_base, 'AB'), exist_ok=True)\n        os.makedirs(os.path.join(output_base, 'flow'), exist_ok=True)\n        os.makedirs(os.path.join(output_base, 'hist_features'), exist_ok=True)\n        \n        prev_frame = None\n        flows = []\n        l_channels = []  # Store L channels for sharpness calculation\n        hist_features = []\n        \n        for i, frame_name in enumerate(frame_paths):\n            frame_path = os.path.join(input_folder, frame_name)\n            frame = cv2.imread(frame_path)\n            lab = cv2.cvtColor(cv2.imread(frame_path), cv2.COLOR_BGR2LAB)\n            lab = cv2.resize(lab, self.resize_dim)\n            \n            # Save components\n            base_name = os.path.splitext(frame_name)[0]\n            np.save(os.path.join(output_base, 'L', f'{base_name}.npy'), lab[:,:,0])\n            np.save(os.path.join(output_base, 'AB', f'{base_name}.npy'), lab[:,:,1:])\n\n            # Process histograms\n            hist_feat = self._process_histograms(frame)\n            hist_features.append(hist_feat.cpu().numpy())\n            np.save(\n                os.path.join(output_base, 'hist_features', f'{base_name}.npy'),\n                hist_feat.cpu().numpy()\n            )\n            \n            l_channels.append(lab[:,:,0])  # Store for sharpness\n            \n            # Flow calculation (existing code)\n            l_channel = lab[:,:,0]\n            current_frame = torch.from_numpy(np.stack([l_channel]*3, axis=2)) \\\n                          .permute(2,0,1).float().unsqueeze(0).to(self.device)\n            \n            if prev_frame is not None:\n                with torch.no_grad():\n                    flow = self.model(prev_frame, current_frame, iters=12)[1]\n                    flows.append(flow.cpu().numpy())\n            prev_frame = current_frame\n        \n        # Calculate temporal sharpness\n        sharpness = self.calculate_temporal_sharpness(l_channels)\n        np.save(os.path.join(output_base, 'sharpness.npy'), sharpness)\n        \n        # Save flows (existing code)\n        np.save(os.path.join(output_base, 'flow.npy'), np.concatenate(flows))\n        np.save(os.path.join(output_base, 'l_frames.npy'), np.array(l_channels))\n        np.save(os.path.join(output_base, 'hist_features.npy'), np.array(hist_features))\n        \n        return {\n            'L': os.path.join(output_base, 'L'),\n            'AB': os.path.join(output_base, 'AB'),\n            'flow': os.path.join(output_base, 'flow.npy'),\n            'sharpness': os.path.join(output_base, 'sharpness.npy'),\n            'hist_features': os.path.join(output_base, 'hist_features')\n        }\n\ndef process_all_videos(root_folder, output_base):\n    \"\"\"Process all video folders in root directory\"\"\"\n    video_folders = [f for f in os.listdir(root_folder) \n                    if os.path.isdir(os.path.join(root_folder, f))]\n    \n    results = {}\n    for folder in video_folders:\n        print(f\"Processing {folder}...\")\n        input_path = os.path.join(root_folder, folder)\n        output_path = os.path.join(output_base, folder)\n        results[folder] = processor.process_folder(input_path, output_path)\n    \n    return results\n\ndef prepare_fusion_input(flows, sharpness, l_frames, flow_scaling=0.1, l_scaling=1.0/255.0):\n    \"\"\"\n    Concatenates optical flow, temporal sharpness, and L channels into a unified input tensor.\n    \n    Args:\n        flows: numpy array of shape (n_frames, 1, 2, H, W) \n        sharpness: numpy array of shape (n_frames,)\n        l_frames: numpy array of shape (n_frames, H, W) - grayscale frames\n        flow_scaling: factor to normalize flow magnitudes\n        l_scaling: factor to normalize L channel values (default 1/255)\n        \n    Returns:\n        torch.Tensor of shape (n_frames, 5, H, W) ready for UNet\n        Channel order: [L, Flow_X, Flow_Y, Flow_Magnitude, Sharpness]\n    \"\"\"\n    # Convert to tensors\n    flows = torch.from_numpy(flows).float()\n    sharpness = torch.from_numpy(sharpness).float()\n    l_frames = torch.from_numpy(l_frames).float()\n    \n    # Remove batch dimension if present in flows\n    if flows.dim() == 5:\n        flows = flows.squeeze(1)  # (n_frames, 2, H, W)\n\n    # Pad flows and sharpness with zeros for first frame\n    flows = F.pad(flows, (0, 0, 0, 0, 0, 0, 1, 0))  # (n_frames, 2, H, W)\n    sharpness = F.pad(sharpness, (1, 0))  # (n_frames,)\n    \n    # Normalize inputs\n    flows = flows * flow_scaling\n    l_frames = l_frames * l_scaling  # Typically scale to [0,1]\n    \n    # Calculate flow magnitude\n    flow_magnitude = torch.norm(flows, dim=1, keepdim=True)  # (n_frames, 1, H, W)\n    \n    # Prepare sharpness features\n    sharpness = sharpness.view(-1, 1, 1, 1)  # (n_frames, 1, 1, 1)\n    sharpness_map = sharpness.expand(-1, 1, flows.shape[-2], flows.shape[-1])  # (n_frames, 1, H, W)\n    \n    # Add channel dimension to L frames\n    l_frames = l_frames.unsqueeze(1)  # (n_frames, 1, H, W)\n    \n    # Concatenate all features\n    fusion_input = torch.cat([\n        l_frames,                # L channel: 1 channel\n        flows,                   # Flow XY: 2 channels\n        flow_magnitude,          # Flow magnitude: 1 channel\n        sharpness_map            # Sharpness map: 1 channel\n    ], dim=1)                   # Total: 5 channels\n    \n    return fusion_input  # (n_frames, 5, H, W)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    # Load your precomputed data\n    processor = VideoProcessor()\n    test_result = processor.process_folder(\n        input_folder=\"/kaggle/input/vcdataset/DS/blackswan\",\n        output_base=\"/kaggle/working/test_output\"\n    )\n    flows = np.load(\"/kaggle/working/test_output/flow.npy\")        # Shape: (n_frames-1, 1, 2, H, W)\n    sharpness = np.load(\"/kaggle/working/test_output/sharpness.npy\") # Shape: (n_frames-1,)\n    l_frames = np.load(\"/kaggle/working/test_output/l_frames.npy\")\n    \n    # Prepare fusion input\n    fusion_input = prepare_fusion_input(flows, sharpness, l_frames)\n    \n    print(\"Fusion input shape:\", fusion_input.shape)\n    print(\"Channel order: [L, Flow_X, Flow_Y, Flow_Magnitude, Sharpness]\")\n    print(\"Sample values:\")\n    print(\"L channel:\", fusion_input[0, 0].min().item(), \"to\", fusion_input[0, 0].max().item())\n    print(\"Flow X:\", fusion_input[0, 1].min().item(), \"to\", fusion_input[0, 1].max().item())\n    print(\"Flow Y:\", fusion_input[0, 2].min().item(), \"to\", fusion_input[0, 2].max().item())\n    print(\"Magnitude:\", fusion_input[0, 3].min().item(), \"to\", fusion_input[0, 3].max().item())\n    print(\"Sharpness:\", fusion_input[0, 4].min().item(), \"to\", fusion_input[0, 4].max().item())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# *Core Architecture*","metadata":{}},{"cell_type":"markdown","source":"# TMM","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass ConvGRUCell(nn.Module):\n    def __init__(self, input_channels, hidden_channels, kernel_size=3):\n        super().__init__()\n        self.hidden_channels = hidden_channels\n        \n        # Gates convolutions\n        self.conv_gates = nn.Conv2d(\n            input_channels + hidden_channels, \n            2 * hidden_channels, \n            kernel_size=kernel_size,\n            padding=kernel_size//2,\n            padding_mode='reflect'\n        )\n        \n        self.conv_candidate = nn.Conv2d(\n            input_channels + hidden_channels,\n            hidden_channels,\n            kernel_size=kernel_size,\n            padding=kernel_size//2,\n            padding_mode='reflect'\n        )\n        \n        # Normalization\n        self.norm_gates = nn.GroupNorm(8, 2 * hidden_channels)\n        self.norm_candidate = nn.GroupNorm(8, hidden_channels)\n        \n        # Learnable hidden state initialization\n        self.h0 = nn.Parameter(torch.randn(1, hidden_channels, 1, 1) * 0.02)\n        \n        # Initialize weights\n        self._init_weights()\n    \n    def _init_weights(self):\n        nn.init.xavier_uniform_(self.conv_gates.weight)\n        nn.init.zeros_(self.conv_gates.bias)\n        with torch.no_grad():\n            self.conv_gates.bias[self.hidden_channels:].fill_(-1)\n        nn.init.xavier_uniform_(self.conv_candidate.weight)\n        nn.init.zeros_(self.conv_candidate.bias)\n    \n    def forward(self, x, h_prev=None):\n        if h_prev is None:\n            batch_size, _, height, width = x.shape\n            h_prev = self.h0.expand(batch_size, -1, height, width)\n        \n        combined = torch.cat([x, h_prev], dim=1)\n        \n        # Gates computation\n        gates = self.norm_gates(self.conv_gates(combined))\n        reset_gate, update_gate = torch.sigmoid(gates).chunk(2, 1)\n        \n        # Candidate computation\n        combined_reset = torch.cat([x, reset_gate * h_prev], dim=1)\n        candidate = torch.tanh(self.norm_candidate(self.conv_candidate(combined_reset)))\n        \n        # New hidden state\n        h_new = update_gate * h_prev + (1 - update_gate) * candidate\n        return h_new","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ConvGRUWrapper(nn.Module):\n    def __init__(self, input_channels, hidden_channels, num_layers=1, \n                 dropout=0.0, use_attention=True, debug=False):\n        super().__init__()\n        self.hidden_channels = hidden_channels\n        self.num_layers = num_layers\n        self.debug = debug\n        \n        self.gru_layers = nn.ModuleList([\n            ConvGRUCell(\n                input_channels if i == 0 else hidden_channels,\n                hidden_channels\n            ) for i in range(num_layers)\n        ])\n        \n        self.attention = (\n            nn.Sequential(\n                nn.Conv2d(hidden_channels, 1, kernel_size=1),\n                nn.Sigmoid()\n            ) if use_attention else None\n        )\n        \n        self.dropout = nn.Dropout2d(dropout) if dropout > 0 else nn.Identity()\n        \n    def init_hidden(self, batch_size, height, width):\n        return [\n            self.gru_layers[i].h0.expand(batch_size, -1, height, width)\n            for i in range(self.num_layers)\n        ]\n        \n    def forward(self, x, hidden_states=None):\n        if self.debug:\n            print(\"\\nConvGRUWrapper forward:\")\n            print(f\"Initial input shape: {x.shape}\")\n        \n        # Input shape handling\n        if x.dim() == 4:\n            x = x.unsqueeze(1)\n        if x.dim() != 5:\n            raise ValueError(f\"Input must be [T,C,H,W] or [T,B,C,H,W], got {x.shape}\")\n        \n        T, B, C, H, W = x.shape\n        \n        if self.debug:\n            print(f\"Processed input shape: {x.shape} (T,B,C,H,W)\")\n        \n        # Initialize hidden states\n        if hidden_states is None:\n            hidden_states = [None] * self.num_layers\n            if self.debug:\n                print(\"Initialized all hidden states as None\")\n        \n        outputs = []\n        for t in range(T):\n            if self.debug:\n                print(f\"\\nProcessing time step {t}\")\n            \n            x_t = x[t]\n            new_hidden = []\n            \n            for layer_idx in range(self.num_layers):\n                h = hidden_states[layer_idx]\n                h = self.gru_layers[layer_idx](x_t, h)\n                new_hidden.append(h)\n                x_t = h\n            \n            # Optional attention + dropout\n            out = x_t\n            if self.attention is not None:\n                attn = self.attention(out)\n                if self.debug:\n                    print(f\"Attention weights mean: {attn.mean().item():.4f}\")\n                out = out * attn\n            \n            out = self.dropout(out)\n            outputs.append(out)\n            hidden_states = new_hidden\n            \n            if self.debug:\n                print(f\"Time step {t} output shape: {out.shape}\")\n                if hidden_states[0] is not None:\n                    print(f\"Hidden state mean: {torch.mean(hidden_states[0]).item():.4f}\")\n        \n        return torch.stack(outputs), hidden_states","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gru = ConvGRUWrapper(input_channels=3, \n                    hidden_channels=64, \n                    num_layers=2, \n                    dropout=0.1, \n                    debug=True)\n\nx = torch.randn(10, 3, 32, 32)  # [T,C,H,W]\noutput, hidden = gru(x)  # output: [T,1,64,32,32], hidden: list of [1,64,32,32]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# SEB block for Histogram Fusion","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass SEBlock(nn.Module):\n    \"\"\"Squeeze-and-Excitation block for channel-wise attention with enhanced checks\"\"\"\n    def __init__(self, channels, reduction=8, debug=False):\n        super().__init__()\n        self.debug = debug\n        self.channels = channels\n        self.reduction = reduction\n        \n        # Validate reduction ratio\n        if channels < reduction:\n            raise ValueError(f\"Channels ({channels}) must be >= reduction ({reduction})\")\n        \n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Linear(channels, channels // reduction),\n            nn.ReLU(inplace=True),\n            nn.Linear(channels // reduction, channels),\n            nn.Sigmoid()\n        )\n        \n        # Initialize weights properly\n        self._init_weights()\n\n    def _init_weights(self):\n        for m in self.fc:\n            if isinstance(m, nn.Linear):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        if x.dim() != 4:\n            raise ValueError(f\"Input must be 4D [B,C,H,W], got {x.dim()}D tensor\")\n            \n        b, c, h, w = x.shape\n        if c != self.channels:\n            raise ValueError(f\"Expected {self.channels} channels, got {c}\")\n        \n        y = self.avg_pool(x).view(b, c)\n        y = self.fc(y).view(b, c, 1, 1)\n        \n        if self.debug:\n            print(f\"[SEBlock] Input shape: {x.shape}\")\n            print(f\"Attention weights - min: {y.min().item():.4f}, max: {y.max().item():.4f}, mean: {y.mean().item():.4f}\")\n        \n        return x * y.expand_as(x)\n\n\nclass CrossAttentionFusion(nn.Module):\n    \"\"\"Enhanced cross-attention with dimension checks and debugging\"\"\"\n    def __init__(self, visual_dim, hist_dim, debug=False):\n        super().__init__()\n        self.debug = debug\n        self.visual_dim = visual_dim\n        self.hist_dim = hist_dim\n        \n        # Projection layers with proper initialization\n        self.query = nn.Linear(visual_dim, visual_dim)\n        self.key = nn.Linear(hist_dim, visual_dim)\n        self.value = nn.Linear(hist_dim, visual_dim)\n        self.softmax = nn.Softmax(dim=-1)\n        \n        self._init_weights()\n\n    def _init_weights(self):\n        nn.init.xavier_uniform_(self.query.weight)\n        nn.init.xavier_uniform_(self.key.weight)\n        nn.init.xavier_uniform_(self.value.weight)\n        nn.init.zeros_(self.query.bias)\n        nn.init.zeros_(self.key.bias)\n        nn.init.zeros_(self.value.bias)\n\n    def forward(self, visual_feat, hist_feat):\n        # Input validation\n        if visual_feat.dim() != 4:\n            raise ValueError(f\"Visual features should be 4D [B,C,H,W], got {visual_feat.shape}\")\n        if hist_feat.dim() != 2:\n            raise ValueError(f\"Hist features should be 2D [B,D], got {hist_feat.shape}\")\n            \n        B, C, H, W = visual_feat.shape\n        if C != self.visual_dim:\n            raise ValueError(f\"Visual dim mismatch: expected {self.visual_dim}, got {C}\")\n        if hist_feat.size(1) != self.hist_dim:\n            raise ValueError(f\"Hist dim mismatch: expected {self.hist_dim}, got {hist_feat.size(1)}\")\n\n        # Flatten visual features\n        visual_flat = visual_feat.view(B, C, -1).permute(0, 2, 1)  # [B, HW, C]\n        \n        # Project features\n        q = self.query(visual_flat)  # [B, HW, C]\n        k = self.key(hist_feat).unsqueeze(1)  # [B, 1, C]\n        v = self.value(hist_feat).unsqueeze(1)  # [B, 1, C]\n        \n        # Attention computation\n        scale = (C) ** -0.5  # More stable scaling\n        attn_logits = (q @ k.transpose(-2, -1)) * scale  # [B, HW, 1]\n        attn = self.softmax(attn_logits)\n        output = (attn * v).permute(0, 2, 1).view(B, C, H, W)\n        \n        if self.debug and torch.is_grad_enabled():\n            print(f\"\\n[CrossAttention] Debug:\")\n            print(f\"Visual in: {visual_feat.shape}, Hist in: {hist_feat.shape}\")\n            print(f\"Q mean: {q.mean().item():.4f}, K mean: {k.mean().item():.4f}\")\n            print(f\"Attention weights - min: {attn.min().item():.4f}, max: {attn.max().item():.4f}\")\n            print(f\"Output delta mean: {(output - visual_feat).mean().item():.4f}\")\n        \n        return output + visual_feat  # Residual connection","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# With debugging enabled\nse = SEBlock(channels=64, reduction=8, debug=True)\ncross_attn = CrossAttentionFusion(visual_dim=64, hist_dim=64, debug=True)\n\n# Normal usage\nvisual_feat = torch.randn(2, 64, 32, 32)\nhist_feat = torch.randn(2, 64)\nout = cross_attn(visual_feat, hist_feat)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# UNet","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass SpatialAttention(nn.Module):\n    \"\"\"Spatial attention module with configurable kernel size\"\"\"\n    def __init__(self, kernel_size=7, debug=False):\n        super().__init__()\n        assert kernel_size % 2 == 1, \"Kernel size should be odd for symmetric padding\"\n        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n        self.debug = debug\n        \n    def forward(self, x):\n        if self.debug:\n            print(f\"\\n[SpatialAttention] Input shape: {x.shape}\")\n        \n        # Channel pooling\n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out, _ = torch.max(x, dim=1, keepdim=True)\n        concat = torch.cat([avg_out, max_out], dim=1)\n        \n        attention = torch.sigmoid(self.conv(concat))\n        \n        if self.debug:\n            print(f\"Attention map - min: {attention.min().item():.4f}, max: {attention.max().item():.4f}\")\n        \n        return x * attention\n\nclass ConvBlock(nn.Module):\n    \"\"\"Basic convolutional block with optional downsampling\"\"\"\n    def __init__(self, in_channels, out_channels, downsample=False, debug=False):\n        super().__init__()\n        self.debug = debug\n        self.downsample = downsample\n        \n        stride = 2 if downsample else 1\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, \n                              stride=stride, padding=1, bias=False)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, \n                               padding=1, bias=False)\n        self.norm = nn.GroupNorm(8, out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        \n        # Shortcut connection for downsampling\n        if downsample:\n            self.down = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=2, bias=False),\n                nn.GroupNorm(8, out_channels)\n            )\n        else:\n            self.down = None\n            \n    def forward(self, x):\n        if self.debug:\n            print(f\"\\n[ConvBlock] Input shape: {x.shape}, downsample: {self.downsample}\")\n        \n        identity = x\n        if self.downsample:\n            identity = self.down(x)\n            if self.debug:\n                print(f\"Shortcut output shape: {identity.shape}\")\n        \n        x = self.conv1(x)\n        x = self.norm(x)\n        x = self.relu(x)\n        \n        x = self.conv2(x)\n        x = self.norm(x)\n        \n        if self.downsample:\n            x = x + identity\n            \n        out = self.relu(x)\n        \n        if self.debug:\n            print(f\"Output shape: {out.shape}\")\n        \n        return out\n\nclass UpConvBlock(nn.Module):\n    \"\"\"Upsampling block with skip connections and attention\"\"\"\n    def __init__(self, in_channels, out_channels, skip_channels=None, debug=False):\n        super().__init__()\n        self.debug = debug\n        self.skip_channels = skip_channels if skip_channels is not None else out_channels\n        \n        # Upsampling layer\n        self.up = nn.ConvTranspose2d(\n            in_channels, out_channels, \n            kernel_size=2, \n            stride=2,\n            bias=False\n        )\n        \n        # Main processing block\n        self.conv = ConvBlock(\n            out_channels + self.skip_channels, \n            out_channels,\n            debug=debug\n        )\n        \n        self.attention = SpatialAttention(debug=debug)\n        \n        if self.debug:\n            print(f\"\\n[UpConvBlock] Initialized with:\")\n            print(f\"in_channels: {in_channels}, out_channels: {out_channels}\")\n            print(f\"skip_channels: {self.skip_channels}\")\n\n    def forward(self, x, skip=None):\n        if self.debug:\n            print(f\"\\n[UpConvBlock] Input shape: {x.shape}\")\n            if skip is not None:\n                print(f\"Skip connection shape: {skip.shape}\")\n            else:\n                print(\"No skip connection provided\")\n        \n        # Upsample main input\n        x = self.up(x)\n        if self.debug:\n            print(f\"After upsampling: {x.shape}\")\n        \n        # Process skip connection if available\n        if skip is not None:\n            # Validate skip connection channels\n            if skip.shape[1] != self.skip_channels:\n                raise ValueError(\n                    f\"Skip channels mismatch: expected {self.skip_channels}, got {skip.shape[1]}\"\n                )\n            \n            # Resize if needed (should be rare in properly constructed networks)\n            if skip.shape[2:] != x.shape[2:]:\n                if self.debug:\n                    print(f\"Resizing skip from {skip.shape[2:]} to {x.shape[2:]}\")\n                skip = F.interpolate(\n                    skip, \n                    size=x.shape[2:], \n                    mode='bilinear', \n                    align_corners=False\n                )\n            \n            x = torch.cat([x, skip], dim=1)\n            if self.debug:\n                print(f\"After skip concatenation: {x.shape}\")\n        \n        # Process through conv block\n        x = self.conv(x)\n        if self.debug:\n            print(f\"After conv block: {x.shape}\")\n        \n        # Apply spatial attention\n        out = self.attention(x)\n        \n        if self.debug:\n            print(f\"Final output shape: {out.shape}\")\n        \n        return out","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# With debugging\nup_block = UpConvBlock(64, 32, skip_channels=64, debug=True)\nx = torch.randn(2, 64, 16, 16)\nskip = torch.randn(2, 64, 32, 32)\nout = up_block(x, skip)\n\n# Normal usage\nup_block = UpConvBlock(64, 32)  # Defaults skip_channels=32","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass BidirectionalVideoColorizationNet(nn.Module):\n    def __init__(self, in_channels=5, out_channels=2, base_channels=64, hist_embed_dim=64, debug=False):\n        super().__init__()\n        self.debug = debug\n        self.base_channels = base_channels\n        self.hist_embed_dim = hist_embed_dim\n\n        # ================ ENCODER ================\n        self.encoder = nn.ModuleDict({\n            'enc1': ConvBlock(in_channels, base_channels, downsample=True),\n            'enc2': ConvBlock(base_channels, base_channels*2, downsample=True),\n            'enc3': ConvBlock(base_channels*2, base_channels*4, downsample=True),\n            'enc4': ConvBlock(base_channels*4, base_channels*8, downsample=True),\n            'enc5': ConvBlock(base_channels*8, base_channels*16, downsample=True)\n        })\n\n        # ================ TEMPORAL PROCESSING ================\n        self.temporal = nn.ModuleDict({\n            'temporal_forward': ConvGRUWrapper(\n                input_channels=base_channels*16,\n                hidden_channels=base_channels*8,\n                num_layers=1,\n                debug=debug\n            ),\n            'temporal_backward': ConvGRUWrapper(\n                input_channels=base_channels*16,\n                hidden_channels=base_channels*8,\n                num_layers=1,\n                debug=debug\n            )\n        })\n\n        # ================ HISTOGRAM PROCESSING ================\n        self.hist_proj = nn.ModuleDict({\n            'high': nn.Sequential(\n                nn.Linear(hist_embed_dim, base_channels*8),\n                nn.ReLU(),\n                nn.LayerNorm(base_channels*8)\n            ),\n            'mid': nn.Sequential(\n                nn.Linear(hist_embed_dim, base_channels*4),\n                nn.ReLU(),\n                nn.LayerNorm(base_channels*4)\n            ),\n            'low': nn.Sequential(\n                nn.Linear(hist_embed_dim, base_channels*2),\n                nn.ReLU(),\n                nn.LayerNorm(base_channels*2)\n            )\n        })\n\n        # ================ FUSION MODULES ================\n        self.fusion = nn.ModuleDict({\n            'proj': nn.Conv2d(base_channels*8*2, base_channels*8, 1),\n            'high': CrossAttentionFusion(\n                visual_dim=base_channels*8,\n                hist_dim=base_channels*8,\n                debug=debug\n            ),\n            'mid': SEBlock(base_channels*4, debug=debug),\n            'low': SEBlock(base_channels*2, debug=debug)\n        })\n\n        # ================ FIRST FRAME PROCESSING ================\n        self.first_frame_proc = nn.Sequential(\n            nn.Conv2d(out_channels, base_channels*4, 3, stride=2, padding=1),\n            nn.GroupNorm(8, base_channels*4),\n            nn.ReLU(),\n            nn.Conv2d(base_channels*4, base_channels*8, 3, stride=2, padding=1),\n            nn.GroupNorm(8, base_channels*8),\n            nn.ReLU()\n        )\n\n        # ================ DECODER ================\n        self.decoder = nn.ModuleDict({\n            'dec1': UpConvBlock(base_channels*8, base_channels*4, \n                              skip_channels=base_channels*8, debug=debug),\n            'dec2': UpConvBlock(base_channels*4, base_channels*2, \n                              skip_channels=base_channels*4, debug=debug),\n            'dec3': UpConvBlock(base_channels*2, base_channels, \n                              skip_channels=base_channels*2, debug=debug)\n        })\n\n        # ================ OUTPUT ================\n        self.output = nn.Sequential(\n            nn.Conv2d(base_channels, base_channels, 3, padding=1),\n            nn.GroupNorm(8, base_channels),\n            nn.ReLU(),\n            nn.Conv2d(base_channels, base_channels, 3, padding=1),\n            nn.Conv2d(base_channels, out_channels, 3, padding=1),\n            nn.Tanh()\n        )\n\n        if self.debug:\n            self.print_architecture()\n            \n    def forward(self, x, hist_features, first_frame_ab):\n        \"\"\"Complete forward pass with batch processing and debugging\"\"\"\n        # ===================== INPUT VALIDATION =====================\n        if self.debug:\n            print(\"\\n\" + \"=\"*30 + \" FORWARD PASS \" + \"=\"*30)\n            print(f\"Input frames shape: {x.shape}\")\n            print(f\"Hist features shape: {hist_features.shape}\")\n            print(f\"First frame AB shape: {first_frame_ab.shape}\")\n    \n        # Validate input dimensions\n        if x.dim() != 5:\n            raise ValueError(f\"Input must be 5D [B,T,1,H,W], got {x.shape}\")\n        if hist_features.dim() != 3:\n            raise ValueError(f\"Hist features must be 3D [B,T,D], got {hist_features.shape}\")\n        if first_frame_ab.dim() != 5 or first_frame_ab.shape[1] != 1:\n            raise ValueError(f\"First frame AB must be [B,1,2,H,W], got {first_frame_ab.shape}\")\n    \n        B, T, C, H, W = x.shape\n        if C != 1:\n            raise ValueError(f\"Input frames should have 1 channel (L), got {C}\")\n    \n        # ===================== PROCESS EACH VIDEO IN BATCH =====================\n        batch_output = []\n        for b in range(B):  # Process each video independently\n            if self.debug:\n                print(f\"\\nProcessing video {b+1}/{B}\")\n    \n            # Get current video data\n            video_L = x[b]  # [T,1,H,W]\n            video_hist = hist_features[b]  # [T,D]\n            video_first_ab = first_frame_ab[b]  # [1,2,H,W]\n    \n            # ===================== ENCODER PASS =====================\n            encoder_features = []\n            skip_connections = {'e4': [], 'e3': [], 'e2': [], 'e1': []}\n    \n            for t in range(T):\n                if self.debug:\n                    print(f\"  Frame {t+1}/{T}\")\n    \n                x_t = video_L[t].unsqueeze(0)  # [1,1,H,W]\n    \n                # Encoder forward pass\n                e1 = self.encoder['enc1'](x_t)\n                e2 = self.encoder['enc2'](e1)\n                e3 = self.encoder['enc3'](e2)\n                e4 = self.encoder['enc4'](e3)\n                e5 = self.encoder['enc5'](e4)\n    \n                if self.debug:\n                    print(f\"    Encoder features:\")\n                    print(f\"    - e1: {e1.shape}\")\n                    print(f\"    - e5: {e5.shape}\")\n    \n                encoder_features.append(e5.squeeze(0))  # Remove batch dim\n                skip_connections['e4'].append(e4.squeeze(0))\n                skip_connections['e3'].append(e3.squeeze(0))\n                skip_connections['e2'].append(e2.squeeze(0))\n                skip_connections['e1'].append(e1.squeeze(0))\n    \n            encoder_features = torch.stack(encoder_features)  # [T,1024,H/32,W/32]\n    \n            # ===================== HISTOGRAM PROCESSING =====================\n            hist_high = self.hist_proj['high'](video_hist)  # [T,512]\n            hist_mid = self.hist_proj['mid'](video_hist)    # [T,256]\n            hist_low = self.hist_proj['low'](video_hist)    # [T,128]\n    \n            if self.debug:\n                print(\"\\n  Histogram projections:\")\n                print(f\"    - high: {hist_high.shape}\")\n                print(f\"    - mid: {hist_mid.shape}\")\n                print(f\"    - low: {hist_low.shape}\")\n    \n            # ===================== TEMPORAL PROCESSING =====================\n            # First frame initialization\n            init_state = self.first_frame_proc(video_first_ab)  # [1,512,H/4,W/4]\n            init_state = F.adaptive_avg_pool2d(init_state, (H//32, W//32))  # [1,512,H/32,W/32]\n    \n            # Forward pass\n            forward_out, _ = self.temporal['temporal_forward'](\n                encoder_features.unsqueeze(1))  # [T,1,512,H/32,W/32]\n            forward_out = forward_out.squeeze(1)  # [T,512,H/32,W/32]\n            forward_out[0] = init_state.squeeze(0)  # Initialize first frame\n    \n            # Backward pass\n            backward_out, _ = self.temporal['temporal_backward'](\n            torch.flip(encoder_features, [0]).unsqueeze(1))  # [T,1,512,H/32,W/32]\n            backward_out = torch.flip(backward_out.squeeze(1), [0])  # [T,512,H/32,W/32]\n            backward_out[-1] = init_state.squeeze(0)\n    \n            if self.debug:\n                print(\"\\n  Temporal processing:\")\n                print(f\"    - Forward out: {forward_out.shape}\")\n                print(f\"    - Backward out: {backward_out.shape}\")\n    \n            # ===================== FUSION AND DECODING =====================\n            video_output = []\n            for t in range(T):\n                if self.debug:\n                    print(f\"  Decoding frame {t+1}/{T}\")\n    \n                # Bidirectional fusion\n                fused = torch.cat([\n                    forward_out[t].unsqueeze(0),\n                    backward_out[t].unsqueeze(0)\n                ], dim=1)  # [1,1024,H/32,W/32]\n                fused = self.fusion['proj'](fused)  # [1,512,H/32,W/32]\n    \n                # High-level fusion with attention\n                fused_high = self.fusion['high'](\n                    fused, \n                    hist_high[t].unsqueeze(0)  # [1,512]\n                )\n    \n                # Get skip connections\n                e4_t = skip_connections['e4'][t].unsqueeze(0)  # [1,512,H/16,W/16]\n                e3_t = skip_connections['e3'][t].unsqueeze(0)  # [1,256,H/8,W/8]\n                e2_t = skip_connections['e2'][t].unsqueeze(0)  # [1,128,H/4,W/4]\n                e1_t = skip_connections['e1'][t].unsqueeze(0)  # [1,64,H/2,W/2]\n    \n                # Decoder with skip connections\n                d1 = self.decoder['dec1'](fused_high, e4_t)  # [1,256,H/16,W/16]\n                \n                # Mid-level fusion\n                hist_mid_t = hist_mid[t].view(1, 256, 1, 1).expand(-1, -1, *d1.shape[2:])\n                d1 = d1 * self.fusion['mid'](hist_mid_t)\n    \n                d2 = self.decoder['dec2'](d1, e3_t)  # [1,128,H/8,W/8]\n                \n                # Low-level fusion\n                hist_low_t = hist_low[t].view(1, 128, 1, 1).expand(-1, -1, *d2.shape[2:])\n                d2 = d2 * self.fusion['low'](hist_low_t)\n    \n                d3 = self.decoder['dec3'](d2, e2_t)  # [1,64,H/4,W/4]\n    \n                # Shallow feature refinement\n                e1_t_resized = F.interpolate(e1_t, size=d3.shape[2:], mode='bilinear')\n                d3_refined = d3 + e1_t_resized\n    \n                # Final output\n                ab = F.interpolate(d3_refined, size=(H,W), mode='bilinear')\n                ab = self.output(ab)  # [1,2,H,W]\n    \n                if t == 0:  # Ensure first frame matches input exactly\n                    ab = video_first_ab\n    \n                if self.debug:\n                    print(f\"    Output AB range: {ab.min().item():.2f} to {ab.max().item():.2f}\")\n    \n                video_output.append(ab.squeeze(0))  # [2,H,W]\n    \n            batch_output.append(torch.stack(video_output))  # [T,2,H,W]\n    \n        return torch.stack(batch_output)  # [B,T,2,H,W]\n\n    def print_architecture(self):\n        \"\"\"Print model architecture for debugging\"\"\"\n        print(\"\\n\" + \"=\"*50)\n        print(\"Bidirectional Video Colorization Network\")\n        print(\"=\"*50)\n        print(f\"Base channels: {self.base_channels}\")\n        print(f\"Histogram embedding dim: {self.hist_embed_dim}\")\n        print(\"\\nEncoder:\")\n        for name, module in self.encoder.items():\n            print(f\"- {name}: {module}\")\n        print(\"\\nTemporal Processing:\")\n        for name, module in self.temporal.items():\n            print(f\"- {name}: {module}\")\n        print(\"\\nDecoder:\")\n        for name, module in self.decoder.items():\n            print(f\"- {name}: {module}\")\n        print(\"=\"*50 + \"\\n\")\n\n    def debug_shapes(self, T=5, H=256, W=256):\n        \"\"\"Print expected tensor shapes for debugging\"\"\"\n        print(\"\\n\" + \"=\"*50)\n        print(\"Expected Tensor Shapes (Debug Mode)\")\n        print(\"=\"*50)\n        print(f\"Input frames: [{T},5,{H},{W}]\")\n        print(f\"Hist features: [{T},{self.hist_embed_dim}]\")\n        print(f\"First frame AB: [1,2,{H},{W}]\")\n            \n        print(\"\\nEncoder Features:\")\n        print(f\"enc1 out: [{T},{self.base_channels},{H//2},{W//2}]\")\n        print(f\"enc5 out: [{T},{self.base_channels*16},{H//32},{W//32}]\")\n            \n        print(\"\\nTemporal Processing:\")\n        print(f\"Forward out: [{T},{self.base_channels*8},{H//32},{W//32}]\")\n            \n        print(\"\\nDecoder Features:\")\n        print(f\"dec1 out: [{T},{self.base_channels*4},{H//16},{W//16}]\")\n        print(f\"Final AB: [{T},2,{H},{W}]\")\n        print(\"=\"*50 + \"\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize model\nmodel = BidirectionalVideoColorizationNet(debug=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install git+https://github.com/moskomule/sam.pytorch.git","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"Total trainable parameters: {count_parameters(model):,}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class VideoColorizationPipeline:\n    def __init__(self, model_config, train_config):\n        # Initialize your original model\n        self.model = BidirectionalVideoColorizationNet(**model_config)\n        \n        # Add performance wrappers (NO architecture changes)\n        self.scaler = GradScaler(enabled=train_config['use_amp'])\n        self.ema = ExponentialMovingAverage(self.model.parameters(), \n                                          decay=train_config['ema_decay'])\n        \n        # SAM + Lookahead (wraps existing optimizer)\n        base_optimizer = optim.AdamW(self.model.parameters(), \n                                    lr=train_config['lr'])\n        self.optimizer = SAM(base_optimizer, rho=train_config['sam_rho'])\n        self.optimizer = Lookahead(self.optimizer)\n        \n        # Enhanced loss function (wraps original outputs)\n        self.loss_fn = WarpConsistencyLoss(\n            l2_weight=train_config['l2_weight'],\n            warp_weight=train_config['warp_weight']\n        )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Clone the repo manually\n!git clone https://github.com/alphadl/lookahead.pytorch.git\n\n# Copy the optimizer to your working directory (optional, but clean)\n!cp lookahead.pytorch/lookahead.py lookahead_optimizer.py\nfrom lookahead_optimizer import Lookahead","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install torch-ema","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install sam-pytorch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.cuda.amp import GradScaler, autocast\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\nfrom torch_ema import ExponentialMovingAverage\nfrom sam import SAM  # pip install sam-pytorch\n\nclass EnhancedTrainer:\n    def __init__(self, model, train_dir, val_dir, config):\n        self.model = model\n        self.config = config\n        self.device = torch.device(config['device'])\n        \n        # Data loading\n        self.train_dataset = VideoFrameDataset(train_dir)\n        self.val_dataset = VideoFrameDataset(val_dir)\n        self.train_loader = torch.utils.data.DataLoader(\n            self.train_dataset,\n            batch_size=config['batch_size'],\n            shuffle=True,\n            num_workers=4,\n            pin_memory=True\n        )\n        self.val_loader = torch.utils.data.DataLoader(\n            self.val_dataset,\n            batch_size=1,  # Process videos one at a time for validation\n            num_workers=2,\n            pin_memory=True\n        )\n\n        # Optimization setup\n        base_optimizer = optim.AdamW(\n            model.parameters(),\n            lr=config['lr'],\n            weight_decay=config['weight_decay']\n        )\n        \n        # SAM + Lookahead\n        self.optimizer = SAM(\n            base_optimizer, \n            model.parameters(),\n            rho=config['sam_rho'],\n            adaptive=config['sam_adaptive']\n        )\n        self.optimizer = Lookahead(\n            self.optimizer,\n            k=config['lookahead_steps'],\n            alpha=config['lookahead_alpha']\n        )\n        \n        # Learning rate scheduling\n        self.scheduler = CosineAnnealingWarmRestarts(\n            self.optimizer,\n            T_0=config['restart_period'],\n            T_mult=config['restart_multiplier'],\n            eta_min=config['min_lr']\n        )\n        \n        # Mixed precision and EMA\n        self.scaler = GradScaler(enabled=config['use_amp'])\n        self.ema = ExponentialMovingAverage(model.parameters(), decay=config['ema_decay'])\n        \n        # Loss functions\n        self.l1_loss = nn.L1Loss()\n        self.mse_loss = nn.MSELoss()\n        \n        # Metrics tracking\n        self.metrics = {\n            'train': {'loss': [], 'psnr': [], 'ssim': []},\n            'val': {'psnr': [], 'ssim': []}\n        }\n\n    def compute_metrics(self, pred, target):\n        \"\"\"Calculate PSNR and SSIM for AB channels\"\"\"\n        pred_np = pred.detach().cpu().numpy()\n        target_np = target.detach().cpu().numpy()\n        \n        batch_psnr = []\n        batch_ssim = []\n        \n        for i in range(pred_np.shape[0]):\n            p = pred_np[i].transpose(1,2,0)\n            t = target_np[i].transpose(1,2,0)\n            \n            batch_psnr.append(psnr(t, p, data_range=2.0))\n            batch_ssim.append(ssim(t, p, multichannel=True, \n                               data_range=2.0, channel_axis=-1))\n        \n        return np.mean(batch_psnr), np.mean(batch_ssim)\n\n    def train_epoch(self, epoch):\n        self.model.train()\n        epoch_loss = 0\n        epoch_psnr = 0\n        epoch_ssim = 0\n        \n        for batch in tqdm(self.train_loader, desc=f\"Epoch {epoch}\"):\n            L = batch['L'].to(self.device)  # [B,T,1,H,W]\n            AB = batch['AB'].to(self.device)  # [B,T,2,H,W]\n            first_ab = batch['first_ab'].to(self.device)  # [B,1,2,H,W]\n            flows = batch['flows'].to(self.device)  # [B,T-1,2,H,W]\n            \n            # SAM requires closure for two forward-backward passes\n            def closure():\n                self.optimizer.zero_grad()\n                with autocast(enabled=self.config['use_amp']):\n                    pred_ab = self.model(L, first_ab, flows)\n                    # Lock first frame AB\n                    pred_ab[:,0] = first_ab.squeeze(1)\n                    \n                    # Combined loss\n                    l1_loss = self.l1_loss(pred_ab, AB)\n                    warp_loss = self.compute_warp_loss(pred_ab, flows)\n                    loss = l1_loss + self.config['warp_weight'] * warp_loss\n                \n                self.scaler.scale(loss).backward()\n                return loss\n            \n            # SAM optimization step\n            loss = self.optimizer.step(closure)\n            self.scaler.step(self.optimizer)\n            self.scaler.update()\n            self.ema.update()\n            \n            # Calculate metrics\n            with torch.no_grad():\n                pred_ab = self.model(L, first_ab, flows)\n                pred_ab[:,0] = first_ab.squeeze(1)\n                batch_psnr, batch_ssim = self.compute_metrics(pred_ab, AB)\n            \n            epoch_loss += loss.item()\n            epoch_psnr += batch_psnr\n            epoch_ssim += batch_ssim\n        \n        # Store metrics\n        self.metrics['train']['loss'].append(epoch_loss / len(self.train_loader))\n        self.metrics['train']['psnr'].append(epoch_psnr / len(self.train_loader))\n        self.metrics['train']['ssim'].append(epoch_ssim / len(self.train_loader))\n        \n        # LR schedule step\n        self.scheduler.step()\n\n    def compute_warp_loss(self, pred_ab, flows):\n        \"\"\"Temporal consistency loss using optical flow\"\"\"\n        loss = 0\n        B, T = pred_ab.shape[:2]\n        \n        for t in range(1, T):\n            warped = self.warp(pred_ab[:,t-1], flows[:,t-1])\n            loss += F.l1_loss(warped, pred_ab[:,t])\n        \n        return loss / (T - 1)\n\n    def warp(self, ab, flow):\n        \"\"\"Differentiable warping of AB channels\"\"\"\n        # Implement flow warping (simplified example)\n        B, C, H, W = ab.shape\n        grid = self.flow_to_grid(flow)\n        return F.grid_sample(ab, grid, padding_mode='border')\n\n    def validate(self):\n        self.model.eval()\n        val_psnr = 0\n        val_ssim = 0\n        \n        with torch.no_grad(), self.ema.average_parameters():\n            for batch in tqdm(self.val_loader, desc=\"Validating\"):\n                L = batch['L'].to(self.device)\n                AB = batch['AB'].to(self.device)\n                first_ab = batch['first_ab'].to(self.device)\n                flows = batch['flows'].to(self.device)\n                \n                with autocast(enabled=self.config['use_amp']):\n                    pred_ab = self.model(L, first_ab, flows)\n                    pred_ab[:,0] = first_ab.squeeze(1)\n                \n                batch_psnr, batch_ssim = self.compute_metrics(pred_ab, AB)\n                val_psnr += batch_psnr\n                val_ssim += batch_ssim\n        \n        # Store validation metrics\n        self.metrics['val']['psnr'].append(val_psnr / len(self.val_loader))\n        self.metrics['val']['ssim'].append(val_ssim / len(self.val_loader))\n        \n        print(f\"Validation PSNR: {self.metrics['val']['psnr'][-1]:.2f}, \"\n              f\"SSIM: {self.metrics['val']['ssim'][-1]:.4f}\")\n\n    def train(self, epochs):\n        for epoch in range(1, epochs + 1):\n            self.train_epoch(epoch)\n            self.validate()\n            \n            # Print metrics\n            print(f\"Train Loss: {self.metrics['train']['loss'][-1]:.4f}, \"\n                  f\"PSNR: {self.metrics['train']['psnr'][-1]:.2f}, \"\n                  f\"SSIM: {self.metrics['train']['ssim'][-1]:.4f}\")\n            \n            # Save checkpoint\n            if epoch % self.config['save_interval'] == 0:\n                self.save_checkpoint(epoch)\n\n    def save_checkpoint(self, epoch):\n        state = {\n            'epoch': epoch,\n            'model_state': self.model.state_dict(),\n            'optimizer_state': self.optimizer.state_dict(),\n            'scheduler_state': self.scheduler.state_dict(),\n            'ema_state': self.ema.state_dict(),\n            'metrics': self.metrics\n        }\n        torch.save(state, f\"checkpoint_epoch{epoch}.pth\")\n\n# Configuration\nconfig = {\n    'device': 'cuda',\n    'lr': 3e-4,\n    'min_lr': 1e-6,\n    'batch_size': 4,\n    'epochs': 100,\n    'weight_decay': 1e-4,\n    'use_amp': True,\n    'sam_rho': 0.05,\n    'sam_adaptive': True,\n    'lookahead_steps': 5,\n    'lookahead_alpha': 0.5,\n    'restart_period': 20,\n    'restart_multiplier': 1,\n    'ema_decay': 0.999,\n    'warp_weight': 0.3,\n    'save_interval': 5\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom torch.utils.data import DataLoader\n\n# Create dummy dataset (5-frame synthetic videos)\nclass DummyDataset(torch.utils.data.Dataset):\n    def __init__(self, num_videos=5, num_frames=5, size=(256,256)):\n        self.num_videos = num_videos\n        self.num_frames = num_frames\n        self.size = size\n        \n    def __len__(self):\n        return self.num_videos\n    \n    def __getitem__(self, idx):\n        # Synthetic L channel (grayscale)\n        L = torch.rand(self.num_frames, 1, *self.size)  # [T,1,H,W]\n        \n        # Synthetic AB channels (color)\n        AB = torch.rand(self.num_frames, 2, *self.size) * 2 - 1  # [-1,1]\n        \n        # First frame AB is fixed\n        first_ab = AB[0].unsqueeze(0)  # [1,2,H,W]\n        \n        # Synthetic flow (between frames)\n        flows = torch.rand(self.num_frames-1, 2, *self.size)  # [T-1,2,H,W]\n        \n        return {\n            'L': L.float(),\n            'AB': AB.float(),\n            'first_ab': first_ab.float(),\n            'flows': flows.float(),\n            'video_name': f\"dummy_video_{idx}\"\n        }\n\n# Test configuration\nconfig = {\n    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n    'lr': 3e-4,\n    'batch_size': 2,\n    'use_amp': False,  # Disable for dummy test\n    'sam_rho': 0.05,\n    'restart_period': 5,\n    'ema_decay': 0.99,\n    'warp_weight': 0.3\n}\n\n# def test_pipeline():\n#     print(\"=== Starting Dummy Test ===\")\n#     print(f\"Using device: {config['device']}\")\n    \n#     # 1. Initialize model\n#     print(\"\\n1. Initializing model...\")\n#     model = BidirectionalVideoColorizationNet(\n#         in_channels=5,  # L + AB + flow\n#         out_channels=2,\n#         base_channels=16,  # Smaller for testing\n#         debug=True\n#     ).to(config['device'])\n#     print(\"Model initialized successfully!\")\n#     print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n    \n#     # 2. Create dummy data\n#     print(\"\\n2. Creating dummy data...\")\n#     dataset = DummyDataset(num_videos=4, num_frames=5)\n#     loader = DataLoader(dataset, batch_size=config['batch_size'])\n#     sample = next(iter(loader))\n#     print(f\"Batch shapes:\")\n#     print(f\"- L: {sample['L'].shape} (input luminance)\")\n#     print(f\"- AB: {sample['AB'].shape} (target color)\")\n#     print(f\"- first_ab: {sample['first_ab'].shape} (fixed first frame)\")\n#     print(f\"- flows: {sample['flows'].shape} (optical flow)\")\n    \n#     # 3. Test forward pass\n#     print(\"\\n3. Testing forward pass...\")\n#     with torch.no_grad():\n#         output = model(\n#             sample['L'].to(config['device']),\n#             sample['first_ab'].to(config['device']),\n#             sample['flows'].to(config['device'])\n#         )\n#     print(\"Forward pass successful!\")\n#     print(f\"Output shape: {output.shape} (should match AB shape)\")\n\n#     # Check whether first frame is preserved (t == 0)\n#     first_frame_preserved = torch.allclose(\n#         output[:, 0],  # predicted AB for first frame in each video\n#         sample['first_ab'].squeeze(1).to(config['device']),\n#         atol=1e-4\n#     )\n#     print(f\"First frame AB preserved: {first_frame_preserved}\")\n    \n#     # 4. Test training step\n#     print(\"\\n4. Testing training step...\")\n#     trainer = EnhancedTrainer(model, None, None, config)  # No real data\n#     loss = trainer.optimizer.step(lambda: trainer.compute_loss(\n#         output, \n#         sample['AB'].to(config['device']), \n#         sample['flows'].to(config['device'])\n#     ))\n#     print(f\"Training step completed with loss: {loss.item():.4f}\")\n    \n#     # 5. Test validation metrics\n#     print(\"\\n5. Testing validation metrics...\")\n#     psnr, ssim = trainer.compute_metrics(\n#         output, \n#         sample['AB'].to(config['device'])\n#     )\n#     print(f\"PSNR: {psnr:.2f} (higher is better)\")\n#     print(f\"SSIM: {ssim:.4f} (0-1, higher is better)\")\n    \n#     # 6. Verify EMA\n#     print(\"\\n6. Testing EMA...\")\n#     base_weight = model.encoder['enc1'].conv1.weight.data.clone()\n#     print(f\"Original first weight mean: {base_weight.mean().item():.4f}\")\n#     with trainer.ema.average_parameters():\n#         ema_weight = model.encoder['enc1'].conv1.weight.data.mean().item()\n#     print(f\"EMA weight mean: {ema_weight:.4f}\")\n#     weights_differ = not torch.allclose(\n#         base_weight, \n#         model.encoder['enc1'].conv1.weight.data,\n#         atol=1e-6\n#     )\n#     print(f\"Values differ: {weights_differ}\")\n    \n#     print(\"\\n=== All tests passed! ===\")\n\n\n# if __name__ == \"__main__\":\n#     test_pipeline()\n# Test with dummy data\ndef test_forward_pass():\n    B, T, H, W = 2, 5, 256, 256  # Batch size, frames, height, width\n    model = BidirectionalVideoColorizationNet(debug=True)\n    \n    # Create dummy inputs\n    x = torch.rand(B, T, 1, H, W)  # L channels\n    hist = torch.rand(B, T, 64)    # Hist features\n    first_ab = torch.rand(B, 1, 2, H, W)  # First frame AB\n    \n    # Run forward pass\n    output = model(x, hist, first_ab)\n    \n    # Verify outputs\n    assert output.shape == (B, T, 2, H, W), f\"Wrong output shape: {output.shape}\"\n    assert torch.allclose(output[:,0], first_ab.squeeze(1)), \"First frame not preserved\"\n    print(\"Forward pass test successful!\")\n\ntest_forward_pass()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.cuda.amp import GradScaler, autocast\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\nfrom tqdm import tqdm\nimport numpy as np\n\n# Fixed imports - install with: pip install sam-pytorch lookahead-pytorch torch-ema\nfrom lookahead_pytorch import Lookahead  # Changed from 'lookahead'\nfrom torch_ema import ExponentialMovingAverage\nfrom sam import SAM  # pip install sam-pytorch\n\n# You'll need to implement these metrics or install scikit-image\ntry:\n    from skimage.metrics import peak_signal_noise_ratio as psnr\n    from skimage.metrics import structural_similarity as ssim\nexcept ImportError:\n    print(\"Warning: scikit-image not found. Install with: pip install scikit-image\")\n    # Fallback simple implementations\n    def psnr(target, pred, data_range=2.0):\n        mse = np.mean((target - pred) ** 2)\n        return 20 * np.log10(data_range / np.sqrt(mse))\n    \n    def ssim(target, pred, multichannel=True, data_range=2.0, channel_axis=-1):\n        # Simplified SSIM - replace with proper implementation\n        return 0.9  # Placeholder\n\nclass EnhancedTrainer:\n    def __init__(self, model, train_dir, val_dir, config):\n        self.model = model\n        self.config = config\n        self.device = torch.device(config['device'])\n        \n        # Data loading - you'll need to implement VideoFrameDataset\n        self.train_dataset = VideoFrameDataset(train_dir)\n        self.val_dataset = VideoFrameDataset(val_dir)\n        self.train_loader = torch.utils.data.DataLoader(\n            self.train_dataset,\n            batch_size=config['batch_size'],\n            shuffle=True,\n            num_workers=4,\n            pin_memory=True\n        )\n        self.val_loader = torch.utils.data.DataLoader(\n            self.val_dataset,\n            batch_size=1,  # Process videos one at a time for validation\n            num_workers=2,\n            pin_memory=True\n        )\n\n        # Optimization setup\n        base_optimizer = optim.AdamW(\n            model.parameters(),\n            lr=config['lr'],\n            weight_decay=config['weight_decay']\n        )\n        \n        # SAM + Lookahead - Fixed parameter passing\n        self.optimizer = SAM(\n            base_optimizer, \n            rho=config['sam_rho'],\n            adaptive=config['sam_adaptive']\n        )\n        self.optimizer = Lookahead(\n            self.optimizer,\n            k=config['lookahead_steps'],\n            alpha=config['lookahead_alpha']\n        )\n        \n        # Learning rate scheduling\n        self.scheduler = CosineAnnealingWarmRestarts(\n            self.optimizer,\n            T_0=config['restart_period'],\n            T_mult=config['restart_multiplier'],\n            eta_min=config['min_lr']\n        )\n        \n        # Mixed precision and EMA\n        self.scaler = GradScaler(enabled=config['use_amp'])\n        self.ema = ExponentialMovingAverage(model.parameters(), decay=config['ema_decay'])\n        \n        # Loss functions\n        self.l1_loss = nn.L1Loss()\n        self.mse_loss = nn.MSELoss()\n        \n        # Metrics tracking\n        self.metrics = {\n            'train': {'loss': [], 'psnr': [], 'ssim': []},\n            'val': {'psnr': [], 'ssim': []}\n        }\n\n    def compute_metrics(self, pred, target):\n        \"\"\"Calculate PSNR and SSIM for AB channels\"\"\"\n        pred_np = pred.detach().cpu().numpy()\n        target_np = target.detach().cpu().numpy()\n        \n        batch_psnr = []\n        batch_ssim = []\n        \n        for i in range(pred_np.shape[0]):\n            p = pred_np[i].transpose(1,2,0)\n            t = target_np[i].transpose(1,2,0)\n            \n            batch_psnr.append(psnr(t, p, data_range=2.0))\n            batch_ssim.append(ssim(t, p, multichannel=True, \n                               data_range=2.0, channel_axis=-1))\n        \n        return np.mean(batch_psnr), np.mean(batch_ssim)\n\n    def train_epoch(self, epoch):\n        self.model.train()\n        epoch_loss = 0\n        epoch_psnr = 0\n        epoch_ssim = 0\n        \n        for batch in tqdm(self.train_loader, desc=f\"Epoch {epoch}\"):\n            L = batch['L'].to(self.device)  # [B,T,1,H,W]\n            AB = batch['AB'].to(self.device)  # [B,T,2,H,W]\n            first_ab = batch['first_ab'].to(self.device)  # [B,1,2,H,W]\n            flows = batch['flows'].to(self.device)  # [B,T-1,2,H,W]\n            \n            # SAM requires closure for two forward-backward passes\n            def closure():\n                self.optimizer.zero_grad()\n                with autocast(enabled=self.config['use_amp']):\n                    pred_ab = self.model(L, first_ab, flows)\n                    # Lock first frame AB\n                    pred_ab[:,0] = first_ab.squeeze(1)\n                    \n                    # Combined loss\n                    l1_loss = self.l1_loss(pred_ab, AB)\n                    warp_loss = self.compute_warp_loss(pred_ab, flows)\n                    loss = l1_loss + self.config['warp_weight'] * warp_loss\n                \n                self.scaler.scale(loss).backward()\n                return loss\n            \n            # SAM optimization step\n            loss = self.optimizer.step(closure)\n            self.scaler.step(self.optimizer)\n            self.scaler.update()\n            self.ema.update()\n            \n            # Calculate metrics\n            with torch.no_grad():\n                pred_ab = self.model(L, first_ab, flows)\n                pred_ab[:,0] = first_ab.squeeze(1)\n                batch_psnr, batch_ssim = self.compute_metrics(pred_ab, AB)\n            \n            epoch_loss += loss.item()\n            epoch_psnr += batch_psnr\n            epoch_ssim += batch_ssim\n        \n        # Store metrics\n        self.metrics['train']['loss'].append(epoch_loss / len(self.train_loader))\n        self.metrics['train']['psnr'].append(epoch_psnr / len(self.train_loader))\n        self.metrics['train']['ssim'].append(epoch_ssim / len(self.train_loader))\n        \n        # LR schedule step\n        self.scheduler.step()\n\n    def compute_warp_loss(self, pred_ab, flows):\n        \"\"\"Temporal consistency loss using optical flow\"\"\"\n        loss = 0\n        B, T = pred_ab.shape[:2]\n        \n        for t in range(1, T):\n            warped = self.warp(pred_ab[:,t-1], flows[:,t-1])\n            loss += F.l1_loss(warped, pred_ab[:,t])\n        \n        return loss / (T - 1)\n\n    def flow_to_grid(self, flow):\n        \"\"\"Convert optical flow to sampling grid\"\"\"\n        B, C, H, W = flow.shape\n        \n        # Create base grid\n        x = torch.arange(W, dtype=torch.float32, device=flow.device)\n        y = torch.arange(H, dtype=torch.float32, device=flow.device)\n        x, y = torch.meshgrid(x, y, indexing='xy')\n        \n        # Normalize to [-1, 1]\n        x = 2.0 * x / (W - 1) - 1.0\n        y = 2.0 * y / (H - 1) - 1.0\n        \n        # Add flow\n        flow_x = flow[:, 0]  # [B, H, W]\n        flow_y = flow[:, 1]  # [B, H, W]\n        \n        # Normalize flow\n        flow_x = 2.0 * flow_x / (W - 1)\n        flow_y = 2.0 * flow_y / (H - 1)\n        \n        # Create grid\n        grid_x = x.unsqueeze(0) + flow_x\n        grid_y = y.unsqueeze(0) + flow_y\n        \n        grid = torch.stack([grid_x, grid_y], dim=-1)  # [B, H, W, 2]\n        \n        return grid\n\n    def warp(self, ab, flow):\n        \"\"\"Differentiable warping of AB channels\"\"\"\n        grid = self.flow_to_grid(flow)\n        return F.grid_sample(ab, grid, padding_mode='border', align_corners=True)\n\n    def validate(self):\n        self.model.eval()\n        val_psnr = 0\n        val_ssim = 0\n        \n        with torch.no_grad(), self.ema.average_parameters():\n            for batch in tqdm(self.val_loader, desc=\"Validating\"):\n                L = batch['L'].to(self.device)\n                AB = batch['AB'].to(self.device)\n                first_ab = batch['first_ab'].to(self.device)\n                flows = batch['flows'].to(self.device)\n                \n                with autocast(enabled=self.config['use_amp']):\n                    pred_ab = self.model(L, first_ab, flows)\n                    pred_ab[:,0] = first_ab.squeeze(1)\n                \n                batch_psnr, batch_ssim = self.compute_metrics(pred_ab, AB)\n                val_psnr += batch_psnr\n                val_ssim += batch_ssim\n        \n        # Store validation metrics\n        self.metrics['val']['psnr'].append(val_psnr / len(self.val_loader))\n        self.metrics['val']['ssim'].append(val_ssim / len(self.val_loader))\n        \n        print(f\"Validation PSNR: {self.metrics['val']['psnr'][-1]:.2f}, \"\n              f\"SSIM: {self.metrics['val']['ssim'][-1]:.4f}\")\n\n    def train(self, epochs):\n        for epoch in range(1, epochs + 1):\n            self.train_epoch(epoch)\n            self.validate()\n            \n            # Print metrics\n            print(f\"Train Loss: {self.metrics['train']['loss'][-1]:.4f}, \"\n                  f\"PSNR: {self.metrics['train']['psnr'][-1]:.2f}, \"\n                  f\"SSIM: {self.metrics['train']['ssim'][-1]:.4f}\")\n            \n            # Save checkpoint\n            if epoch % self.config['save_interval'] == 0:\n                self.save_checkpoint(epoch)\n\n    def save_checkpoint(self, epoch):\n        state = {\n            'epoch': epoch,\n            'model_state': self.model.state_dict(),\n            'optimizer_state': self.optimizer.state_dict(),\n            'scheduler_state': self.scheduler.state_dict(),\n            'ema_state': self.ema.state_dict(),\n            'metrics': self.metrics\n        }\n        torch.save(state, f\"checkpoint_epoch{epoch}.pth\")\n\n\n# Placeholder for VideoFrameDataset - you'll need to implement this\nclass VideoFrameDataset(torch.utils.data.Dataset):\n    def __init__(self, data_dir):\n        self.data_dir = data_dir\n        # Add your dataset implementation here\n        pass\n    \n    def __len__(self):\n        # Return dataset size\n        return 100  # Placeholder\n    \n    def __getitem__(self, idx):\n        # Return sample with keys: 'L', 'AB', 'first_ab', 'flows'\n        # This is a placeholder - implement your actual data loading\n        return {\n            'L': torch.randn(8, 1, 256, 256),  # [T, 1, H, W]\n            'AB': torch.randn(8, 2, 256, 256),  # [T, 2, H, W]\n            'first_ab': torch.randn(1, 2, 256, 256),  # [1, 2, H, W]\n            'flows': torch.randn(7, 2, 256, 256)  # [T-1, 2, H, W]\n        }\n\n\n# Placeholder for model - you'll need to implement this\nclass BidirectionalVideoColorizationNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Add your model architecture here\n        self.dummy = nn.Linear(1, 1)  # Placeholder\n    \n    def forward(self, L, first_ab, flows):\n        # Return predicted AB channels\n        B, T = L.shape[:2]\n        return torch.randn(B, T, 2, 256, 256)  # Placeholder\n\n\n# Configuration\nconfig = {\n    'device': 'cuda',\n    'lr': 3e-4,\n    'min_lr': 1e-6,\n    'batch_size': 4,\n    'epochs': 100,\n    'weight_decay': 1e-4,\n    'use_amp': True,\n    'sam_rho': 0.05,\n    'sam_adaptive': True,\n    'lookahead_steps': 5,\n    'lookahead_alpha': 0.5,\n    'restart_period': 20,\n    'restart_multiplier': 1,\n    'ema_decay': 0.999,\n    'warp_weight': 0.3,\n    'save_interval': 5\n}\n\n# Usage\nif __name__ == \"__main__\":\n    model = BidirectionalVideoColorizationNet()\n    trainer = EnhancedTrainer(model, \"data/train\", \"data/val\", config)\n    trainer.train(config['epochs'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class WarpConsistencyLoss(nn.Module):\n    def __init__(self, l2_weight=1.0, warp_weight=0.5):\n        super().__init__()\n        self.l2 = nn.MSELoss()\n        self.l2_weight = l2_weight\n        self.warp_weight = warp_weight\n\n    def forward(self, pred_ab, target_ab, flows):\n        # Original L2 loss\n        l2_loss = self.l2(pred_ab, target_ab)\n        \n        # New temporal consistency term\n        warp_loss = 0\n        for t in range(1, pred_ab.size(0)):\n            warped = self.warp(pred_ab[t-1].unsqueeze(0), flows[t-1])\n            warp_loss += F.l1_loss(warped, pred_ab[t].unsqueeze(0))\n        \n        return self.l2_weight*l2_loss + self.warp_weight*warp_loss/pred_ab.size(0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"config = {\n    # Model config (exactly matches your original)\n    'model': {\n        'in_channels': 5,\n        'out_channels': 2,\n        'base_channels': 64,\n        'hist_embed_dim': 64,\n        'debug': False\n    },\n    \n    # Training enhancements\n    'train': {\n        'use_amp': True,\n        'ema_decay': 0.999,\n        'sam_rho': 0.05,\n        'l2_weight': 1.0,\n        'warp_weight': 0.3,\n        'lr': 3e-4\n    }\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_step(self, x, hist, first_ab, targets, flows):\n    def closure():\n        self.optimizer.zero_grad()\n        with autocast():\n            outputs = self.model(x, hist, first_ab)\n            loss = self.loss_fn(outputs, targets, flows)\n        self.scaler.scale(loss).backward()\n        return loss\n    \n    # SAM step\n    loss = self.optimizer.step(closure)\n    \n    # EMA update\n    self.ema.update()\n    \n    return loss","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ColorizationLoss(nn.Module):\n    def __init__(self, warp_weight=0.5):\n        super().__init__()\n        self.l2_loss = nn.MSELoss()\n        self.warp_weight = warp_weight\n        \n    def warp_frame(self, ab, flow):\n        \"\"\"Differentiable warping using grid_sample\"\"\"\n        # flow: (1,2,H,W) - must be displacement vectors\n        H, W = ab.shape[-2:]\n        grid = self.flow_to_grid(flow)  # Convert to sampling grid\n        return F.grid_sample(ab, grid, mode='bilinear', padding_mode='border', align_corners=True)\n    \n    def flow_to_grid(self, flow):\n        # Generate coordinate grid\n        H, W = flow.shape[-2:]\n        y, x = torch.meshgrid(torch.arange(H), torch.arange(W))\n        grid = torch.stack((x, y), dim=0).float().to(flow.device)  # (2,H,W)\n        \n        # Add flow displacements (normalize to [-1,1])\n        grid = grid + flow.squeeze(0)  # Add displacement\n        grid[:, 0, :] = 2.0 * grid[:, 0, :] / max(W - 1, 1) - 1.0  # X coord\n        grid[:, 1, :] = 2.0 * grid[:, 1, :] / max(H - 1, 1) - 1.0  # Y coord\n        return grid.permute(1, 2, 0).unsqueeze(0)  # (1,H,W,2)\n\n    def forward(self, pred_ab, target_ab, prev_data=None, next_data=None):\n        color_loss = self.l2_loss(pred_ab, target_ab)\n        \n        warp_loss = 0\n        if prev_data is not None and next_data is not None:\n            prev_pred, prev_flow = prev_data\n            next_pred, next_flow = next_data\n            \n            # Warp using PyTorch (differentiable)\n            warped_prev = self.warp_frame(prev_pred, prev_flow)\n            warped_next = self.warp_frame(next_pred, next_flow)\n            \n            warp_loss = F.l1_loss(pred_ab, warped_prev) + F.l1_loss(pred_ab, warped_next)\n            warp_loss = warp_loss / 2  # Average two directions\n            \n        return color_loss + self.warp_weight * warp_loss\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def visualize_results(input_frames, output_ab, num_frames=3):\n    plt.figure(figsize=(18, 6*num_frames))\n    \n    for i in range(min(num_frames, len(input_frames))):\n        # Get normalized channels\n        l_channel = input_frames[i,0].cpu().numpy() * 100  # 0-100\n        a_channel = output_ab[i,0].cpu().numpy()  # Already -127 to 127\n        b_channel = output_ab[i,1].cpu().numpy()  # Already -127 to 127\n        \n        # Create LAB image\n        lab_image = np.stack([l_channel, a_channel, b_channel], axis=-1).astype(np.float32)\n        \n        # Convert to RGB\n        rgb_image = cv2.cvtColor(lab_image, cv2.COLOR_LAB2RGB)\n        rgb_image = np.clip(rgb_image, 0, 1)  # Ensure valid range\n        \n        # Plotting (same as before)\n        plt.subplot(num_frames, 4, 4*i+1)\n        plt.imshow(l_channel/100, cmap='gray')  # Show normalized L\n        plt.title(f\"Frame {i} - L Channel\")\n        plt.axis('off')\n        \n        plt.subplot(num_frames, 4, 4*i+2)\n        plt.imshow(a_channel, cmap='coolwarm', vmin=-127, vmax=127)\n        plt.title(f\"Frame {i} - A Channel\")\n        plt.colorbar()\n        plt.axis('off')\n        \n        plt.subplot(num_frames, 4, 4*i+3)\n        plt.imshow(b_channel, cmap='coolwarm', vmin=-127, vmax=127)\n        plt.title(f\"Frame {i} - B Channel\")\n        plt.colorbar()\n        plt.axis('off')\n        \n        plt.subplot(num_frames, 4, 4*i+4)\n        plt.imshow(rgb_image)\n        plt.title(f\"Frame {i} - Colorized\")\n        plt.axis('off')\n    \n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport numpy as np\n\ndef get_ab_channels_tensor(image_path, size=256):\n    \"\"\"\n    Loads image, converts to Lab color space, returns a and b channels with shape [1, 2, H, W].\n\n    Args:\n        image_path (str): Path to image file.\n        size (int): Desired height and width of output (default 256).\n\n    Returns:\n        np.ndarray: Array of shape [1, 2, H, W] containing a and b channels.\n    \"\"\"\n    # Read image\n    image = cv2.imread(image_path)\n    if image is None:\n        raise ValueError(f\"Could not load image at {image_path}\")\n\n    # Resize to desired size\n    image = cv2.resize(image, (size, size))\n\n    # Convert to Lab color space\n    lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2Lab)\n\n    # Split channels\n    _, a_channel, b_channel = cv2.split(lab_image)\n\n    # Stack a and b, shape will be [2, H, W]\n    ab = np.stack([a_channel, b_channel], axis=0)\n\n    # Add batch dimension -> [1, 2, H, W]\n    ab = np.expand_dims(ab, axis=0)\n\n    return ab.astype(np.float32)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Garbage","metadata":{}},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n# # 1. Load your preprocessed data\n# flows = np.load(\"/kaggle/working/test_output/flow.npy\")        # Shape: (n_frames-1, 1, 2, H, W)\n# sharpness = np.load(\"/kaggle/working/test_output/sharpness.npy\") # Shape: (n_frames-1,)\n# l_frames = np.load(\"/kaggle/working/test_output/l_frames.npy\")   # Shape: (n_frames, H, W)\n\n# # 2. Prepare fusion input\n# fusion_input = prepare_fusion_input(flows, sharpness, l_frames)  # (n_frames, 5, H, W)\n\n# # 3. Initialize model\n# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# model = VideoColorizationStreamer().to(device)\n\n# # 4. Run test prediction\n# with torch.no_grad():\n#     # Select first 8 frames for testing (avoid OOM)\n#     test_input = fusion_input[:8].to(device)\n#     output = model(test_input)\n    \n#     print(\"Input shape:\", test_input.shape)\n#     print(\"Output shape:\", output.shape)\n#     print(\"Output range: [{:.1f}, {:.1f}]\".format(\n#         output.min().item(), output.max().item()))\n\n# # 5. Visual verification\n# def visualize_results(input_frames, ab_output, num_frames=3):\n#     fig, axes = plt.subplots(num_frames, 3, figsize=(15, 5*num_frames))\n    \n#     for i in range(num_frames):\n#         # Input L channel\n#         l_channel = input_frames[i,0].cpu().numpy()\n#         axes[i,0].imshow(l_channel, cmap='gray', vmin=0, vmax=1)\n#         axes[i,0].set_title(f'Frame {i} L channel')\n        \n#         # Predicted AB channels - show each channel separately\n#         ab_vis = ab_output[i].cpu().numpy()\n#         axes[i,1].imshow(ab_vis[0], cmap='coolwarm', vmin=-127, vmax=127)  # A channel\n#         axes[i,1].set_title('Predicted A channel')\n        \n#         # Combined LAB->RGB\n#         L = l_channel * 100  # Scale to [0,100]\n#         AB = ab_vis.transpose(1,2,0)  # Convert to HxWx2\n#         lab = np.dstack((L, AB))\n#         rgb = cv2.cvtColor(lab.astype(np.float32), cv2.COLOR_LAB2RGB)\n#         axes[i,2].imshow(rgb)\n#         axes[i,2].set_title('Colorized Result')\n    \n#     plt.tight_layout()\n#     plt.show()\n\n# # Run visualization\n# visualize_results(test_input, output)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport numpy as np\n\n# 1. Load precomputed data\nflows_np = np.load(\"/kaggle/working/test_output/flow.npy\")        # (n_frames - 1, 1, 2, H, W)\nsharpness_np = np.load(\"/kaggle/working/test_output/sharpness.npy\")  # (n_frames - 1,)\nl_frames_np = np.load(\"/kaggle/working/test_output/l_frames.npy\")    # (n_frames, H, W)\n\n# 2. Convert to tensors (no padding yet)\nflows_tensor = torch.from_numpy(flows_np).float().squeeze(1)      # (n_frames - 1, 2, H, W)\nsharpness_tensor = torch.from_numpy(sharpness_np).float()         # (n_frames - 1,)\nl_frames_tensor = torch.from_numpy(l_frames_np).float()           # (n_frames, H, W)\n\n# 3. Define number of frames to process\nnum_frames = 9  # Total frames including padding, e.g., to predict frames 2-9\n\nassert num_frames <= l_frames_tensor.shape[0], \"Requested more frames than available.\"\n\n# 4. Slice consistent inputs\nflows_slice = flows_tensor[:num_frames - 1]       # (num_frames - 1, 2, H, W)\nsharpness_slice = sharpness_tensor[:num_frames - 1]  # (num_frames - 1,)\nl_slice = l_frames_tensor[:num_frames]            # (num_frames, H, W)\n\n# 5. Prepare fusion input (padding happens inside)\nfusion_input = prepare_fusion_input(flows_slice.numpy(), sharpness_slice.numpy(), l_slice.numpy())\n\nprint(f\"Fusion input shape: {fusion_input.shape}\")  # Expect (num_frames, 5, H, W)\n\n# 6. Load your trained colorization model\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = VideoColorizationNet().to(device)\nmodel.eval()\n\n# 7. Run model on frames\nwith torch.no_grad():\n    test_input = fusion_input.to(device)  # Shape: (num_frames, 5, H, W)\n    output = model(test_input)            # Output: (num_frames, 2, H, W)\n\n    print(f\"Output shape: {output.shape}\")\n    print(f\"Output range: [{output.min().item():.1f}, {output.max().item():.1f}]\")\n\n# 8. Visualize results\nvisualize_results(test_input.cpu(), output.cpu())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class UNetBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1).to(device)\n        self.relu = nn.ReLU().to(device)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1).to(device)\n\n    def forward(self, x):\n        x = x.to(device)\n        x = self.conv1(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        return x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom skimage.color import rgb2lab\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport torchvision.transforms as transforms\n\nclass VideoFramesDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.video_folders = [os.path.join(root_dir, f) for f in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, f))]\n\n    def __len__(self):\n        return len(self.video_folders)\n\n    def __getitem__(self, idx):\n        folder_path = self.video_folders[idx]\n        frame_paths = sorted([os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.png') or f.endswith('.jpg')])\n\n        frames_lab = []\n        for frame_path in frame_paths:\n            frame = Image.open(frame_path).convert('RGB')\n            if self.transform:\n                frame = self.transform(frame)\n\n            frame_np = frame.permute(1, 2, 0).numpy()  # Convert to (H, W, C) format\n            frame_lab = rgb2lab(frame_np).astype(np.float32)\n\n            # Normalize LAB values\n            frame_lab[:, :, 0] = frame_lab[:, :, 0] / 100.0  # Normalize L channel to [0, 1]\n            frame_lab[:, :, 1:] = frame_lab[:, :, 1:] / 127.0  # Normalize ab channels to [-1, 1]\n\n            frame_lab = torch.from_numpy(frame_lab).permute(2, 0, 1)  # Convert back to (C, H, W)\n            frames_lab.append(frame_lab)\n\n        frames_lab = torch.stack(frames_lab)  # Shape: (num_frames, C, H, W)\n\n        # Extract Y (luminance) and ab (chrominance) channels\n        y_frames = frames_lab[:, 0:1, :, :]  # Y channel\n        ab_frames = frames_lab[:, 1:, :, :]  # ab channels\n\n        return y_frames, ab_frames  # Shape: (num_frames, 1, H, W), (num_frames, 2, H, W)\n\n\n# Define transforms\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n])\n\n# Create dataset and dataloader\ndataset = VideoFramesDataset(root_dir='/kaggle/input/vcdataset/DS', transform=transform)\ndataloader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class JFHM(nn.Module):\n    def __init__(self, input_channels, hidden_dim, num_frames=3):\n        super().__init__()\n        self.histogram_extractor = AdaptiveMultiScaleBinning().to(device)\n        self.spatial_attention = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=8).to(device)\n        self.temporal_attention = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=8).to(device)\n        self.num_frames = num_frames\n        self.hidden_dim = hidden_dim\n\n    def forward(self, x, flow_features):\n        batch_size, channels, height, width = x.shape\n\n        # Compute histograms\n        hist_features = self.histogram_extractor(x)  # Shape: (batch_size, num_bins, channels)\n        hist_features = hist_features.view(batch_size, -1, channels).to(device)  # Shape: (batch_size, num_bins, channels)\n\n        # Ensure hist_features has the correct embedding dimension\n        if hist_features.size(-1) != self.hidden_dim:\n            hist_features = hist_features.permute(0, 2, 1)  # Swap dimensions to match expected shape\n            hist_features = F.linear(hist_features, torch.zeros(self.hidden_dim, hist_features.size(-1)).to(device))  # Project to hidden_dim\n\n        # Cross-Attention between histograms and flow features\n        spatial_features, _ = self.spatial_attention(hist_features, hist_features, hist_features)  # Shape: (batch_size, num_bins, hidden_dim)\n\n        # Process flow features\n        temporal_input = torch.cat([flow_features[:, i] for i in range(self.num_frames-1)], dim=1).to(device)  # Shape: (batch_size, num_frames-1, hidden_dim)\n        temporal_features, _ = self.temporal_attention(temporal_input, temporal_input, temporal_input)  # Shape: (batch_size, num_frames-1, hidden_dim)\n\n        # Combine spatial and temporal features\n        combined_features = (spatial_features + temporal_features).to(device)  # Shape: (batch_size, num_frames-1, hidden_dim)\n\n        return combined_features","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class FlowWeightingNetwork(nn.Module):\n    def __init__(self, input_channels, hidden_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(input_channels * 2, hidden_dim)  # Adjust for concatenated flows\n        self.fc2 = nn.Linear(hidden_dim, 1)\n        self.relu = nn.ReLU()\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, forward_flow, backward_flow):\n        # Concatenate forward and backward flows along the channel dimension\n        combined = torch.cat([forward_flow, backward_flow], dim=2)  # Shape: (B, num_frames-1, 4, H, W)\n        combined = combined.permute(0, 1, 3, 4, 2)  # Shape: (B, num_frames-1, H, W, 4)\n\n        # Pass through the weighting network\n        weights = self.relu(self.fc1(combined))  # Shape: (B, num_frames-1, H, W, hidden_dim)\n        weights = self.sigmoid(self.fc2(weights))  # Shape: (B, num_frames-1, H, W, 1)\n\n        # Apply weights to forward and backward flows\n        weighted_forward = forward_flow * weights.permute(0, 1, 4, 2, 3)  # Shape: (B, num_frames-1, 2, H, W)\n        weighted_backward = backward_flow * (1 - weights.permute(0, 1, 4, 2, 3))  # Shape: (B, num_frames-1, 2, H, W)\n\n        # Combine weighted flows\n        combined_flow = weighted_forward + weighted_backward  # Shape: (B, num_frames-1, 2, H, W)\n\n        return combined_flow","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class VideoColorizationUNet(nn.Module):\n    def __init__(self, num_frames=3):\n        super().__init__()\n        self.num_frames = num_frames\n        self.encoder1 = UNetBlock(1, 64)  # Input is Y channel (1 channel)\n        self.encoder2 = UNetBlock(64, 128)\n        self.encoder3 = UNetBlock(128, 256)\n        self.encoder4 = UNetBlock(256, 512)\n        self.bottleneck = JFHM(512, 256, num_frames)\n        self.decoder4 = UNetBlock(1024, 256)  # Adjusted for skip connection\n        self.decoder3 = UNetBlock(512, 128)\n        self.decoder2 = UNetBlock(256, 64)\n        self.decoder1 = UNetBlock(128, 2)  # Output 2 channels for ab\n        self.jfhm1 = JFHM(256, 128, num_frames)\n        self.jfhm2 = JFHM(128, 64, num_frames)\n        self.flow_estimator = OpticalFlowEstimator()\n        self.flow_weighting = FlowWeightingNetwork(input_channels=2, hidden_dim=64)  # Weighting network\n\n    def forward(self, frames):\n        frames = frames.to(device)  # Shape: (B, num_frames, 1, H, W)\n        print(f\"Input frames shape: {frames.shape}\")\n\n        # Handle cases where num_frames = 1\n        print(self.flow_estimator(frames[0], frames[0]).shape)\n        if self.num_frames == 1:\n            # Use the same frame for both inputs (no optical flow)\n            forward_flow = [self.flow_estimator(frames[0], frames[0])]\n            backward_flow = [self.flow_estimator(frames[0], frames[0])]\n        else:\n            # Forward flow: frame[t] -> frame[t+1]\n            forward_flow = [self.flow_estimator(frames[i], frames[i+1]) for i in range(self.num_frames-1)]\n            # Backward flow: frame[t+1] -> frame[t]\n            backward_flow = [self.flow_estimator(frames[i+1], frames[i]) for i in range(self.num_frames-1)]\n\n        # Stack forward and backward flows\n        forward_flow = torch.stack(forward_flow, dim=1)  # Shape: (B, num_frames-1, 2, H, W)\n        backward_flow = torch.stack(backward_flow, dim=1)  # Shape: (B, num_frames-1, 2, H, W)\n        print(f\"Forward flow shape: {forward_flow.shape}\")\n        print(f\"Backward flow shape: {backward_flow.shape}\")\n\n        # Use the weighting network to combine forward and backward flows\n        flow_features = self.flow_weighting(forward_flow, backward_flow)  # Shape: (B, num_frames-1, 2, H, W)\n\n        # Encoder\n        e1 = self.encoder1(frames[:, self.num_frames // 2])  # Use middle frame as reference\n        e2 = self.encoder2(e1)\n        e3 = self.encoder3(e2)\n        e4 = self.encoder4(e3)\n\n        # Bottleneck with flow features\n        b = self.bottleneck(e4, flow_features)  # Shape: (B, num_frames-1, hidden_dim)\n\n        # Decoder with skip connections\n        d4 = self.decoder4(torch.cat([b, e4], dim=1))\n        d3 = self.decoder3(torch.cat([self.jfhm1(d4, flow_features), e3], dim=1))\n        d2 = self.decoder2(torch.cat([self.jfhm2(d3, flow_features), e2], dim=1))\n        d1 = self.decoder1(torch.cat([d2, e1], dim=1))  # Shape: (B, 2, H, W)\n\n        # Repeat the output for all frames\n        predicted_ab = d1.unsqueeze(1).repeat(1, self.num_frames, 1, 1, 1)  # Shape: (B, num_frames, 2, H, W)\n\n        return predicted_ab","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchmetrics import PeakSignalNoiseRatio, StructuralSimilarityIndexMeasure\nfrom torch.cuda.amp import GradScaler, autocast\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import StepLR\nimport logging\n\n# Initialize logging\nlogging.basicConfig(filename='training.log', level=logging.INFO, format='%(asctime)s - %(message)s')\n\n# Ensure device selection\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Define Trainer Class\nclass Trainer:\n    def __init__(self, model, lr=1e-4, checkpoint_dir='checkpoints'):\n        self.model = model.to(device)\n        self.optimizer = Adam(self.model.parameters(), lr=lr)\n        self.criterion = nn.MSELoss().to(device)\n        self.psnr = PeakSignalNoiseRatio().to(device)\n        self.ssim = StructuralSimilarityIndexMeasure().to(device)\n        self.checkpoint_dir = checkpoint_dir\n        os.makedirs(self.checkpoint_dir, exist_ok=True)\n        self.scaler = GradScaler()  # For mixed precision training\n\n    def save_checkpoint(self, epoch, loss, checkpoint_name='checkpoint.pth'):\n        checkpoint_path = os.path.join(self.checkpoint_dir, checkpoint_name)\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': self.model.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict(),\n            'scaler_state_dict': self.scaler.state_dict(),\n            'loss': loss,\n        }, checkpoint_path)\n        print(f\"Checkpoint saved at {checkpoint_path}\")\n\n    def load_checkpoint(self, checkpoint_name='checkpoint.pth'):\n        checkpoint_path = os.path.join(self.checkpoint_dir, checkpoint_name)\n        if os.path.exists(checkpoint_path):\n            checkpoint = torch.load(checkpoint_path, map_location=device)\n            self.model.load_state_dict(checkpoint['model_state_dict'])\n            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n            self.scaler.load_state_dict(checkpoint['scaler_state_dict'])\n            epoch = checkpoint['epoch']\n            loss = checkpoint['loss']\n            print(f\"Checkpoint loaded from {checkpoint_path}\")\n            return epoch, loss\n        else:\n            print(f\"No checkpoint found at {checkpoint_path}\")\n            return 0, float('inf')  # Start from scratch if no checkpoint exists\n\n    def train_step(self, y, ab_gt, predicted_ab):\n        self.model.train()\n        self.optimizer.zero_grad()\n\n        # Mixed precision training\n        with autocast():\n            loss = self.criterion(predicted_ab, ab_gt)\n\n        # Backpropagation with scaling\n        self.scaler.scale(loss).backward()\n        self.scaler.step(self.optimizer)\n        self.scaler.update()\n\n        return loss.item()\n\n    def evaluate(self, y, ab_gt, predicted_ab):\n        self.model.eval()\n        with torch.no_grad():\n            psnr = self.psnr(predicted_ab, ab_gt)\n            ssim = self.ssim(predicted_ab, ab_gt)\n        return psnr.item(), ssim.item()\n\n# # Define Model (Replace with actual implementation)\n# class VideoColorizationUNet(nn.Module):\n#     def __init__(self):\n#         super(VideoColorizationUNet, self).__init__()\n#         self.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n#         self.conv2 = nn.Conv2d(64, 2, kernel_size=3, padding=1)\n\n#     def forward(self, x):\n#         x = torch.relu(self.conv1(x))\n#         x = self.conv2(x)\n#         return x\n\n# Initialize model and move to device\nmodel = VideoColorizationUNet().to(device)\n\n# Apply weight initialization\ndef init_weights(m):\n    if isinstance(m, nn.Conv2d):\n        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n        if m.bias is not None:\n            nn.init.constant_(m.bias, 0)\n\nmodel.apply(init_weights)\n\n# Initialize optimizer and scheduler\noptimizer = Adam(model.parameters(), lr=1e-4)\nscheduler = StepLR(optimizer, step_size=5, gamma=0.1)  # Reduce LR by 0.1 every 5 epochs\n\n# Initialize trainer\ntrainer = Trainer(model, checkpoint_dir='checkpoints')\n\n# Load checkpoint if it exists\nstart_epoch, best_loss = trainer.load_checkpoint()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training loop\nfor epoch in range(start_epoch, 10):\n    for frames_y, frames_ab in dataloader:\n        print(frames_y.shape)  # Shape: (B, num_frames, 1, H, W)\n\n        # Move tensors to device\n        frames_y = frames_y.to(device, non_blocking=True)  # Shape: (B, num_frames, 1, H, W)\n        frames_ab = frames_ab.to(device, non_blocking=True)  # Shape: (B, num_frames, 2, H, W)\n\n        # Predict AB channels for all frames\n        predicted_ab = model(frames_y.squeeze(0))  # Shape: (B, num_frames, 2, H, W)\n\n        # Compute loss and metrics for each sample\n        for i in range(frames_y.shape[0]):\n            # Extract frames for the current sample\n            y_frames = frames_y[i]  # Shape: (num_frames, 1, H, W)\n            ab_frames = frames_ab[i]  # Shape: (num_frames, 2, H, W)\n            pred_ab_frames = predicted_ab[i]  # Shape: (num_frames, 2, H, W)\n\n            # Compute loss\n            loss = trainer.train_step(y_frames[0], ab_frames[0], pred_ab_frames)\n\n            # Compute PSNR and SSIM for each frame in the sample\n            psnr_values = []\n            ssim_values = []\n            for j in range(y_frames.shape[0]):\n                psnr = trainer.psnr(pred_ab_frames[j], ab_frames[j])\n                ssim = trainer.ssim(pred_ab_frames[j], ab_frames[j])\n                psnr_values.append(psnr.item())\n                ssim_values.append(ssim.item())\n\n            # Log metrics for the sample\n            logging.info(f\"Epoch {epoch+1}, Sample {i+1}, Loss: {loss:.4f}, Avg PSNR: {np.mean(psnr_values):.4f}, Avg SSIM: {np.mean(ssim_values):.4f}\")\n\n    # Save checkpoint at the end of each epoch\n    trainer.save_checkpoint(epoch + 1, loss, checkpoint_name=f'checkpoint_epoch_{epoch+1}.pth')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport urllib.request\nimport os\n\n# Download pretrained SIGGRAPH17 model (from Colorization repo)\nmodel_url = \"https://colorizers.s3.us-east-2.amazonaws.com/colorization_release_v2-9b330a0b.pth\"\nmodel_path = \"siggraph17.pth\"\nif not os.path.exists(model_path):\n    urllib.request.urlretrieve(model_url, model_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torchvision import transforms\n\n# Load model\nmodel = torch.hub.load('harvard-visionlab/pytorch-colorization', 'siggraph17', pretrained=False)\nmodel.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\nmodel.eval()\n\ndef colorize_siggraph17(img_path):\n    # Load image (convert to LAB space)\n    img = cv2.imread(img_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) / 255.0\n    img_lab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n    img_l = img_lab[:,:,0]  # Extract L channel (grayscale)\n    \n    # Preprocess\n    img_l = cv2.resize(img_l, (256, 256))\n    img_l = torch.from_numpy(img_l).unsqueeze(0).unsqueeze(0).float()\n    \n    # Predict ab channels\n    with torch.no_grad():\n        pred_ab = model(img_l).cpu().numpy()\n    \n    # Resize and combine with L\n    pred_ab = cv2.resize(pred_ab[0].transpose(1, 2, 0), (img.shape[1], img.shape[0]))\n    pred_lab = np.concatenate([img_lab[:,:,0][:,:,np.newaxis], pred_ab], axis=2)\n    pred_rgb = cv2.cvtColor(pred_lab, cv2.COLOR_LAB2RGB)\n    \n    return pred_rgb","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport cv2\nimport numpy as np\nfrom torchvision import transforms\n\nclass VideoColorizer:\n    def __init__(self, model_path):\n        # Load model\n        self.model = torch.hub.load('harvard-visionlab/pytorch-colorization', \n                                  'siggraph17', pretrained=False)\n        self.model.load_state_dict(torch.load(model_path, \n                                           map_location=torch.device('cpu')))\n        self.model.eval()\n        \n        # Preprocessing transforms\n        self.transform = transforms.Compose([\n            transforms.ToPILImage(),\n            transforms.Resize(256),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.5], std=[0.5])  # For L channel\n        ])\n    \n    def colorize_from_l(self, l_frame, original_size=None):\n        \"\"\"\n        Colorize from preprocessed L channel\n        Args:\n            l_frame: numpy array (H,W) in [0,255] range\n            original_size: (width, height) to resize output to\n        Returns:\n            colorized RGB image\n        \"\"\"\n        # Convert to tensor and preprocess\n        l_tensor = self.transform(l_frame).unsqueeze(0)  # (1,1,H,W)\n        \n        # Predict ab channels\n        with torch.no_grad():\n            pred_ab = self.model(l_tensor).cpu().numpy()[0]  # (2,H,W)\n        \n        # Reshape and resize\n        pred_ab = pred_ab.transpose(1, 2, 0)  # (H,W,2)\n        \n        if original_size is not None:\n            pred_ab = cv2.resize(pred_ab, original_size)\n            l_frame = cv2.resize(l_frame, original_size)\n        \n        # Combine with L channel\n        pred_lab = np.dstack((l_frame, pred_ab))\n        \n        # Convert to RGB\n        pred_rgb = cv2.cvtColor(pred_lab.astype(np.uint8), cv2.COLOR_LAB2RGB)\n        return pred_rgb\n\n# Usage example:\nif __name__ == \"__main__\":\n    # Initialize with your model path\n    colorizer = VideoColorizer(\"path/to/siggraph17.pth\")\n    \n    # Load your preprocessed L frame (from earlier pipeline)\n    l_frame = np.load(\"/kaggle/working/test_output/L/frame_0000.npy\")  # Shape (H,W)\n    \n    # Colorize\n    colorized = colorizer.colorize_from_l(l_frame, original_size=(512, 512))\n    \n    # Display\n    plt.figure(figsize=(10, 5))\n    plt.subplot(1, 2, 1)\n    plt.imshow(l_frame, cmap='gray')\n    plt.title(\"Input L Channel\")\n    \n    plt.subplot(1, 2, 2)\n    plt.imshow(colorized)\n    plt.title(\"Colorized Output\")\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torchvision import transforms\nimport os\nfrom tqdm import tqdm\n\nclass ColorizationModel(nn.Module):\n    \"\"\"A simplified version of the SIGGRAPH17 colorization model\"\"\"\n    def __init__(self):\n        super().__init__()\n        # Define your model architecture here\n        self.model = nn.Sequential(\n            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.ConvTranspose2d(64, 2, kernel_size=4, stride=2, padding=1),\n            nn.Tanh()\n        )\n\n    def forward(self, x):\n        return self.model(x)\n\nclass VideoColorizer:\n    def __init__(self, model_path=None):\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        self.model = ColorizationModel().to(self.device)\n        \n        if model_path and os.path.exists(model_path):\n            self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n        \n        self.model.eval()\n        \n        self.transform = transforms.Compose([\n            transforms.ToPILImage(),\n            transforms.Resize(256),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.5], std=[0.5])\n        ])\n    \n    def colorize_from_l(self, l_frame, original_size=None):\n        \"\"\"\n        Colorize from preprocessed L channel\n        Args:\n            l_frame: numpy array (H,W) in [0,255] range\n            original_size: (width, height) to resize output to\n        Returns:\n            colorized RGB image\n        \"\"\"\n        # Convert to tensor and preprocess\n        l_tensor = self.transform(l_frame).unsqueeze(0).to(self.device)  # (1,1,H,W)\n        \n        # Predict ab channels\n        with torch.no_grad():\n            pred_ab = self.model(l_tensor).cpu().numpy()[0]  # (2,H,W)\n        \n        # Post-process\n        pred_ab = pred_ab.transpose(1, 2, 0)  # (H,W,2)\n        pred_ab = (pred_ab * 127.5).astype(np.float32)  # Scale to [-127, 127]\n        \n        if original_size is not None:\n            pred_ab = cv2.resize(pred_ab, original_size)\n            l_frame = cv2.resize(l_frame, original_size)\n        \n        # Combine with L channel\n        pred_lab = np.dstack((l_frame, pred_ab))\n        pred_rgb = cv2.cvtColor(pred_lab.astype(np.uint8), cv2.COLOR_LAB2RGB)\n        return pred_rgb\n\n    def colorize_video_frames(self, l_folder, output_folder):\n        \"\"\"Colorize all L frames in a folder\"\"\"\n        os.makedirs(output_folder, exist_ok=True)\n        l_files = sorted([f for f in os.listdir(l_folder) if f.endswith('.npy')])\n        \n        for l_file in tqdm(l_files, desc=\"Colorizing frames\"):\n            l_path = os.path.join(l_folder, l_file)\n            l_frame = np.load(l_path)\n            \n            colorized = self.colorize_from_l(l_frame)\n            output_path = os.path.join(output_folder, l_file.replace('.npy', '.png'))\n            cv2.imwrite(output_path, cv2.cvtColor(colorized, cv2.COLOR_RGB2BGR))\n\n# Usage example\nif __name__ == \"__main__\":\n    # Initialize colorizer (without pretrained weights)\n    colorizer = VideoColorizer()\n    \n    # Example: Colorize a single L frame\n    l_frame = np.load(\"/kaggle/working/test_output/L/00000.npy\")  # Replace with your actual L frame\n    colorized = colorizer.colorize_from_l(l_frame)\n    \n    plt.figure(figsize=(10, 5))\n    plt.subplot(1, 2, 1)\n    plt.imshow(l_frame, cmap='gray')\n    plt.title(\"Input L Channel\")\n    \n    plt.subplot(1, 2, 2)\n    plt.imshow(colorized)\n    plt.title(\"Colorized Output\")\n    plt.show()\n    \n    # To process a whole folder:\n    # colorizer.colorize_video_frames(\n    #     l_folder=\"/path/to/L_frames\",\n    #     output_folder=\"/path/to/colorized_output\"\n    # )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install \"numpy<2.0.0\" --force-reinstall","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport torch\nfrom torch import nn\n\nclass BaseColor(nn.Module):\n\tdef __init__(self):\n\t\tsuper(BaseColor, self).__init__()\n\n\t\tself.l_cent = 50.\n\t\tself.l_norm = 100.\n\t\tself.ab_norm = 110.\n\n\tdef normalize_l(self, in_l):\n\t\treturn (in_l-self.l_cent)/self.l_norm\n\n\tdef unnormalize_l(self, in_l):\n\t\treturn in_l*self.l_norm + self.l_cent\n\n\tdef normalize_ab(self, in_ab):\n\t\treturn in_ab/self.ab_norm\n\n\tdef unnormalize_ab(self, in_ab):\n\t\treturn in_ab*self.ab_norm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\n\nclass SIGGRAPHGenerator(BaseColor):\n    def __init__(self, norm_layer=nn.BatchNorm2d, classes=529):\n        super(SIGGRAPHGenerator, self).__init__()\n\n        # Conv1\n        model1=[nn.Conv2d(4, 64, kernel_size=3, stride=1, padding=1, bias=True),]\n        model1+=[nn.ReLU(True),]\n        model1+=[nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=True),]\n        model1+=[nn.ReLU(True),]\n        model1+=[norm_layer(64),]\n        # add a subsampling operation\n\n        # Conv2\n        model2=[nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=True),]\n        model2+=[nn.ReLU(True),]\n        model2+=[nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=True),]\n        model2+=[nn.ReLU(True),]\n        model2+=[norm_layer(128),]\n        # add a subsampling layer operation\n\n        # Conv3\n        model3=[nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1, bias=True),]\n        model3+=[nn.ReLU(True),]\n        model3+=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=True),]\n        model3+=[nn.ReLU(True),]\n        model3+=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=True),]\n        model3+=[nn.ReLU(True),]\n        model3+=[norm_layer(256),]\n        # add a subsampling layer operation\n\n        # Conv4\n        model4=[nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n        model4+=[nn.ReLU(True),]\n        model4+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n        model4+=[nn.ReLU(True),]\n        model4+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n        model4+=[nn.ReLU(True),]\n        model4+=[norm_layer(512),]\n\n        # Conv5\n        model5=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n        model5+=[nn.ReLU(True),]\n        model5+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n        model5+=[nn.ReLU(True),]\n        model5+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n        model5+=[nn.ReLU(True),]\n        model5+=[norm_layer(512),]\n\n        # Conv6\n        model6=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n        model6+=[nn.ReLU(True),]\n        model6+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n        model6+=[nn.ReLU(True),]\n        model6+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n        model6+=[nn.ReLU(True),]\n        model6+=[norm_layer(512),]\n\n        # Conv7\n        model7=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n        model7+=[nn.ReLU(True),]\n        model7+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n        model7+=[nn.ReLU(True),]\n        model7+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n        model7+=[nn.ReLU(True),]\n        model7+=[norm_layer(512),]\n\n        # Conv7\n        model8up=[nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=True)]\n        model3short8=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=True),]\n\n        model8=[nn.ReLU(True),]\n        model8+=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=True),]\n        model8+=[nn.ReLU(True),]\n        model8+=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=True),]\n        model8+=[nn.ReLU(True),]\n        model8+=[norm_layer(256),]\n\n        # Conv9\n        model9up=[nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=True),]\n        model2short9=[nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=True),]\n        # add the two feature maps above        \n\n        model9=[nn.ReLU(True),]\n        model9+=[nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=True),]\n        model9+=[nn.ReLU(True),]\n        model9+=[norm_layer(128),]\n\n        # Conv10\n        model10up=[nn.ConvTranspose2d(128, 128, kernel_size=4, stride=2, padding=1, bias=True),]\n        model1short10=[nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=True),]\n        # add the two feature maps above\n\n        model10=[nn.ReLU(True),]\n        model10+=[nn.Conv2d(128, 128, kernel_size=3, dilation=1, stride=1, padding=1, bias=True),]\n        model10+=[nn.LeakyReLU(negative_slope=.2),]\n\n        # classification output\n        model_class=[nn.Conv2d(256, classes, kernel_size=1, padding=0, dilation=1, stride=1, bias=True),]\n\n        # regression output\n        model_out=[nn.Conv2d(128, 2, kernel_size=1, padding=0, dilation=1, stride=1, bias=True),]\n        model_out+=[nn.Tanh()]\n\n        self.model1 = nn.Sequential(*model1)\n        self.model2 = nn.Sequential(*model2)\n        self.model3 = nn.Sequential(*model3)\n        self.model4 = nn.Sequential(*model4)\n        self.model5 = nn.Sequential(*model5)\n        self.model6 = nn.Sequential(*model6)\n        self.model7 = nn.Sequential(*model7)\n        self.model8up = nn.Sequential(*model8up)\n        self.model8 = nn.Sequential(*model8)\n        self.model9up = nn.Sequential(*model9up)\n        self.model9 = nn.Sequential(*model9)\n        self.model10up = nn.Sequential(*model10up)\n        self.model10 = nn.Sequential(*model10)\n        self.model3short8 = nn.Sequential(*model3short8)\n        self.model2short9 = nn.Sequential(*model2short9)\n        self.model1short10 = nn.Sequential(*model1short10)\n\n        self.model_class = nn.Sequential(*model_class)\n        self.model_out = nn.Sequential(*model_out)\n\n        self.upsample4 = nn.Sequential(*[nn.Upsample(scale_factor=4, mode='bilinear'),])\n        self.softmax = nn.Sequential(*[nn.Softmax(dim=1),])\n\n    def forward(self, input_A, input_B=None, mask_B=None):\n        if(input_B is None):\n            input_B = torch.cat((input_A*0, input_A*0), dim=1)\n        if(mask_B is None):\n            mask_B = input_A*0\n\n        conv1_2 = self.model1(torch.cat((self.normalize_l(input_A),self.normalize_ab(input_B),mask_B),dim=1))\n        conv2_2 = self.model2(conv1_2[:,:,::2,::2])\n        conv3_3 = self.model3(conv2_2[:,:,::2,::2])\n        conv4_3 = self.model4(conv3_3[:,:,::2,::2])\n        conv5_3 = self.model5(conv4_3)\n        conv6_3 = self.model6(conv5_3)\n        conv7_3 = self.model7(conv6_3)\n\n        conv8_up = self.model8up(conv7_3) + self.model3short8(conv3_3)\n        conv8_3 = self.model8(conv8_up)\n        conv9_up = self.model9up(conv8_3) + self.model2short9(conv2_2)\n        conv9_3 = self.model9(conv9_up)\n        conv10_up = self.model10up(conv9_3) + self.model1short10(conv1_2)\n        conv10_2 = self.model10(conv10_up)\n        out_reg = self.model_out(conv10_2)\n\n        conv9_up = self.model9up(conv8_3) + self.model2short9(conv2_2)\n        conv9_3 = self.model9(conv9_up)\n        conv10_up = self.model10up(conv9_3) + self.model1short10(conv1_2)\n        conv10_2 = self.model10(conv10_up)\n        out_reg = self.model_out(conv10_2)\n\n        return self.unnormalize_ab(out_reg)\n\ndef siggraph17(pretrained=True):\n    model = SIGGRAPHGenerator()\n    if(pretrained):\n        import torch.utils.model_zoo as model_zoo\n        model.load_state_dict(model_zoo.load_url('https://colorizers.s3.us-east-2.amazonaws.com/siggraph17-df00044c.pth',map_location='cpu',check_hash=True))\n    return model\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport cv2\nimport torch\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nimport urllib.request\nimport os\n\nclass VideoColorizer:\n    def __init__(self):\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        self.model = self._load_correct_model()\n        self.model.eval()\n    \n    def _load_correct_model(self):\n        # Load compatible weights (tested version)\n        model_url = \"https://colorizers.s3.us-east-2.amazonaws.com/colorization_release_v2-9b330a0b.pth\"\n        model_path = \"siggraph17_compatible.pth\"\n        \n        if not os.path.exists(model_path):\n            print(\"Downloading compatible weights...\")\n            urllib.request.urlretrieve(model_url, model_path)\n        \n        # Use simplified model that matches these weights\n        model = torch.hub.load('kazuto1011/colorization-pytorch', 'siggraph17', pretrained=False)\n        state_dict = torch.load(model_path, map_location=self.device, weights_only=True)\n        model.load_state_dict(state_dict)\n        return model.to(self.device)\n\n    def colorize_L_frame(self, L):\n        \"\"\"Colorize a single L-frame (numpy array)\"\"\"\n        # Convert to tensor and normalize\n        L = L.astype(np.float32)\n        L_tensor = torch.from_numpy(L).unsqueeze(0).unsqueeze(0).float().to(self.device)\n        L_tensor = (L_tensor / 255.0 * 100) - 50  # Normalize to [-50, 50]\n        \n        # Predict AB channels\n        with torch.no_grad():\n            pred_ab = self.model(L_tensor).cpu().numpy()[0]\n        \n        # Resize AB to match original dimensions\n        pred_ab = cv2.resize(pred_ab.transpose(1, 2, 0), (L.shape[1], L.shape[0]))\n        \n        # Combine with L and convert to RGB\n        pred_lab = np.concatenate([L[:,:,np.newaxis], pred_ab], axis=2)\n        pred_rgb = cv2.cvtColor(pred_lab.astype(np.float32), cv2.COLOR_LAB2RGB)\n        return np.clip(pred_rgb * 255, 0, 255).astype(np.uint8)\n\n# Usage\nif __name__ == \"__main__\":\n    colorizer = VideoColorizer()\n    \n    # Load your L-frame\n    L = np.load(\"/kaggle/working/test_output/L/00000.npy\")  # Replace with your path\n    \n    # Colorize\n    colorized = colorizer.colorize_L_frame(L)\n    \n    # Display\n    plt.figure(figsize=(12, 6))\n    plt.subplot(1, 2, 1)\n    plt.imshow(L, cmap='gray')\n    plt.title(\"Original L-frame\")\n    plt.axis('off')\n    \n    plt.subplot(1, 2, 2)\n    plt.imshow(colorized)\n    plt.title(\"Colorized\")\n    plt.axis('off')\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import urllib.request\nmodel_url = \"https://colorizers.s3.us-east-2.amazonaws.com/colorization_release_v2-9b330a0b.pth\"\nmodel_path = \"siggraph17.pth\"\nurllib.request.urlretrieve(model_url, model_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# In your loss function\ndef loss_fn(pred_ab, target_ab):\n    # L1 loss for color accuracy\n    color_loss = F.l1_loss(pred_ab, target_ab)\n    \n    # Temporal smoothness loss (between consecutive frames)\n    flow_loss = F.mse_loss(pred_ab[1:] - pred_ab[:-1], \n                          target_ab[1:] - target_ab[:-1])\n    \n    return color_loss + 0.3 * flow_loss  # Weighted sum","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}